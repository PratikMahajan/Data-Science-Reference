{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data and Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Big Data\n",
    "\n",
    "“Every day, we create 2.5 quintillion bytes of data—so much that 90% of the data in the world today has been created in the last two years alone.” _IBM. 2012. “What is big data?”  Retrieved from [http://www-01.ibm.com/software/data/bigdata/](http://www-01.ibm.com/software/data/bigdata/)_ Due to the growing complexity of digital social networks and the huge quantity of data they produce daily, it’s crucial to  deal with “big data” efficiently. A number of tools and technologies (Map-Reduce, NoSQL, Hadoop, Hive, cloud computing, parallel processing, clustering, MPP, virtualization, large grid environments, and so on) have been developed to store and process big data. While big-data technologies have established the ability to collect and process large amounts of data, most organizations struggle with understanding the data and taking advantage of its value. According to an Economist report: “Extracting value from big data remains elusive for many organizations. For most companies today, data are abundant and readily available, but not well used.” _Economist. 2012. Economist Intelligence Unit Retrieved from [http://www.eiu.com/](http://www.eiu.com/)_\n",
    "\n",
    "A central issue with “big data” is that not all of the data will be relevant or useful in solving a particular problem and will often add noise instead. However, the purpose of the present research is to better measure the system state of social networks, and this knowledge of state can help with a central issue: which data and algorithms are relevant to a particular problem? As our basic algorithm is to find tags in text, look up entropies, and sum, it is easily parallelizable and scales well to “big data.” \n",
    "\n",
    "Central issues with big data:\n",
    "\n",
    "* Finding the signal in the noise\n",
    "* Data silos (isolated non-ingregrated data)\n",
    "* Inaccurate data\n",
    "* Algorithms that scale\n",
    "* Echo-chamber effect (Much of big data comes from the web. Whenever the source of information for a big data analysis is itself a product of big data, opportunities for vicious cycles abound)\n",
    "* Too many (spurious) correlations\n",
    "* Storage\n",
    "* Less structured, heterogenous  \n",
    "* Recognition (identifying what's what in the data) \n",
    "* Discovery (efficient ways to find the specific data that can solve a particular problem) \n",
    "* Semantics (effective and efficient ways to contextualize the \"meaning\"  of the data)\n",
    "* Visualization --effective ways to analyze and visualize n-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data munging\n",
    "\n",
    "Data munging (sometimes referred to as data wrangling) is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning \n",
    "\n",
    " * Save original data\n",
    " * Identify missing data\n",
    " * Identify placeholder data (e.g. 0's for NA's) \n",
    " * Identify outliers\n",
    " * Check for overall plausibility and errors (e.g., typos, unreasonable ranges)\n",
    " * Identify highly correlated variables\n",
    " * Identify variables with (nearly) no variance\n",
    " * Identify variables with strange names or values\n",
    " * Check variable classes (eg. Characters vs factors)\n",
    " * Remove/transform some variables (maybe your model does not like categorial variables)\n",
    " * Rename some variables or values (if not all data is useful)\n",
    " * Check some overall pattern (statistical/ numerical summaries)\n",
    " * Possibly center/scale variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "In the this lesson, we look statistical and graphical techniques summarize their main characteristics of one, two variable, and multi-variable data sets. To find relationships amongst variables and to find the variables which are most interesting for a particular analysis task.\n",
    "\n",
    "Rationale: Exploratory data analysis helps one understand the data, to form and change new theories, and decide which techniques are appropriate for analysis. After a model is finished, exploratory data analysis can look for patterns in these data that may have been missed by the original hypothesis tests. Successful exploratory analyses help the researcher modify theories and refine the analysis.\n",
    "\n",
    "\n",
    "* Suggest hypotheses about the causes of observed phenomena\n",
    " * Assess assumptions on which statistical inference will be based\n",
    " * Support the selection of appropriate statistical tools and techniques\n",
    " * Provide a basis for further data collection through surveys or experiments\n",
    "\n",
    "_Five methods that are must have_:\n",
    "  \n",
    " * Five number summaries (mean/median, min, max, q1, q3)\n",
    " * Histograms \n",
    " * Line charts\n",
    " * Box and whisker plots\n",
    " * Pairwise scatterplots (scatterplot matrices)\n",
    " \n",
    " * What values do you see?\n",
    " * What distributions do you see?\n",
    " * What relationships do you see?\n",
    " * What relationships do you think might benefit the prediction problem?\n",
    "\n",
    "\n",
    "* Answer the following questions for the data in each column:\n",
    "    * How is the data distributed?\n",
    "    * Test distribution assumptions (e.G. Normal distributions or skewed?)\n",
    "    * What are the summary statistics?\n",
    "    * Are there anomalies/outliers?\n",
    "* Identify useful raw data & transforms (e.g. log(x))\n",
    "* Identify data quality problems\n",
    "* Identify outliers\n",
    "* Identify subsets of interest\n",
    "* Suggest functional relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x116721ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxgAAALFCAYAAABAqkUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107a22588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "sns.pairplot(iris, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability, random variables and probability distributions\n",
    "\n",
    "* Probability\n",
    "* Random variables\n",
    "* Probability distributions\n",
    "    * Uniform\n",
    "    * Normal\n",
    "    * Binomial\n",
    "    * Poisson     \n",
    "    * Fat Tailed\n",
    "    \n",
    "## Probability\n",
    "\n",
    "* Probability is a measure of the likelihood of a random phenomenon or chance behavior.  Probability describes the long-term proportion with which a certain outcome will occur in situations with short-term uncertainty. \n",
    "* Probability is expressed in numbers between 0 and 1.  Probability = 0 means the event never happens; probability = 1 means it always happens.\n",
    "* The total probability of all possible event always sums to 1. \n",
    "\n",
    "## Sample Space\n",
    "\n",
    "* Coin Toss ={head,tail} \n",
    "* Two coins S = {HH, HT, TH, TT}\n",
    "* Inspecting a part ={good,bad}\n",
    "* Rolling a die S ={1,2,3,4,5,6}\n",
    "\n",
    "## Random Variables\n",
    "\n",
    "In probability and statistics, a random variable,  or stochastic variable is a variable whose value is subject to variations due to chance (i.e. it can take on a range of values)\n",
    "\n",
    "\n",
    "* Coin Toss ={head,tail} \n",
    "* Rolling a die S ={1,2,3,4,5,6}\n",
    "\n",
    "Discrete Random Variables\n",
    "\n",
    "* Random variables (RVs) which may take on only a countable number of distinct values\n",
    "E.g. the total number of tails X you get if you flip 100 coins\n",
    "* X is a RV with arity k if it can take on exactly one value out of {x1, …, xk}\n",
    "E.g. the possible values that X can take on are 0, 1, 2, …, 100\n",
    "\n",
    "Continuous Random Variables\n",
    "\n",
    "* Probability density function (pdf) instead of probability mass function (pmf)\n",
    "* A pdf is any function f(x) that describes the probability density in terms of the input variable x.\n",
    "\n",
    "\n",
    "\n",
    "## Probability distributions\n",
    "\n",
    "* We use probability distributions because they model data in real world.\n",
    "* They allow us to calculate what to expect and therefore understand what is unusual.\n",
    "* They also provide insight in to the process in which real world data may have been generated.\n",
    "* Many machine learning algorithms have assumptions based on certain probability distributions.\n",
    "\n",
    "_Cumulative distribution function_\n",
    "\n",
    "A probability distribution Pr on the real line is determined by the probability of a scalar random variable X being in a half-open interval (-$\\infty$, x], the probability distribution is completely characterized by its cumulative distribution function:\n",
    "\n",
    "$$\n",
    " F(x) = \\Pr[X \\leq x] \\quad \\forall \\quad x \\in R .\n",
    "$$\n",
    "\n",
    "\n",
    "## Uniform Distribution\n",
    "\n",
    "$$\n",
    "X \\equiv U[a,b]\n",
    "$$\n",
    "\n",
    "$$\n",
    " f(x) = \\frac{1}{b-a} \\quad for \\quad a \\lt x \\lt b\n",
    "$$\n",
    "\n",
    "$$\n",
    " f(x) = 0 \\quad for \\quad a \\leq x  \\quad or  \\quad \\geq b\n",
    "$$\n",
    "\n",
    "$$\n",
    " F(x) = \\frac{x-a}{b-a} \\quad for \\quad a \\leq x \\lt b\n",
    "$$\n",
    "\n",
    "$$\n",
    "F(x) = 0 \\quad for \\quad x  \\lt a  \\quad \n",
    " F(x) = 1 \\quad for \\quad x  \\geq b\n",
    "$$\n",
    "\n",
    "![image Uniform Distribution\"](http://nikbearbrown.com/YouTube/MachineLearning/M01/Uniform_A.png)\n",
    "\n",
    "_Continuous Uniform Distribution_\n",
    "\n",
    "In probability theory and statistics, the continuous uniform distribution or rectangular distribution is a family of symmetric probability distributions such that for each member of the family, all intervals of the same length on the distribution's support are equally probable.\n",
    "\n",
    "- from [Uniform distribution (continuous  Wikipedia)](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "    \n",
    "\n",
    "![image continuous  Uniform Distribution\"](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Uniform_Distribution_PDF_SVG.svg/375px-Uniform_Distribution_PDF_SVG.svg.png)\n",
    "![image continuous  Uniform Distribution\"](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Uniform_cdf.svg/375px-Uniform_cdf.svg.png)\n",
    "\n",
    "_Discrete Uniform Distribution_\n",
    "\n",
    "In probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution whereby a finite number of values are equally likely to be observed; every one of n values has equal probability 1/n. Another way of saying \"discrete uniform distribution\" would be \"a known, finite number of outcomes equally likely to happen\".\n",
    "\n",
    "- from [Uniform distribution (discrete)  Wikipedia)](https://en.wikipedia.org/wiki/Uniform_distribution_(discrete))\n",
    "    \n",
    "\n",
    "![image Uniform distribution (discrete) \"](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Uniform_discrete_pmf_svg.svg/488px-Uniform_discrete_pmf_svg.svg.png)\n",
    "![imageUniform distribution (discrete) \"](https://upload.wikimedia.org/wikipedia/commons/thumb/7/77/Dis_Uniform_distribution_CDF.svg/488px-Dis_Uniform_distribution_CDF.svg.png)\n",
    "\n",
    "## Normal Distribution\n",
    "\n",
    "In probability theory, the normal (or Gaussian) distribution is a very common continuous probability distribution. The normal distribution is remarkably useful because of the central limit theorem. In its most general form, under mild conditions, it states that averages of random variables independently drawn from independent distributions are normally distributed. Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal. \n",
    "\n",
    "- from [Normal Distribution - Wikipedia)](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "   \n",
    "\n",
    "$$\n",
    "X \\sim \\quad N(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    " f(x) = \\frac{1}{\\sigma \\sqrt {2\\pi }} e^{-\\frac{( x - \\mu)^2}{2\\sigma^2}} \\quad \n",
    "$$\n",
    "\n",
    "\n",
    "![image Normal Distribution\"](http://nikbearbrown.com/YouTube/MachineLearning/M01/Normal_Distribution_A.png)\n",
    "\n",
    "\n",
    "![image Normal Distribution  \"](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/525px-Normal_Distribution_PDF.svg.png)\n",
    "\n",
    "Normal cumulative distribution function\n",
    "![image Normal cumulative distribution function \"](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Normal_Distribution_CDF.svg/525px-Normal_Distribution_CDF.svg.png)\n",
    "\n",
    "\n",
    "_Properties of normal distribution_\n",
    "\n",
    "- symmetrical, unimodal, and bell-shaped\n",
    "- on average, the error component will equal zero, the error above and below the mean will cancel out\n",
    "- Z-Score is a statistical measurement is (above/below) the mean of the data\n",
    "- important characteristics about z scores:\n",
    "  1. mean of z scores is 0\n",
    "  2. standard deviation of a standardized variable is always 1\n",
    "  3. the linear transformation does not change the _form_ of the distribution\n",
    "\n",
    "\n",
    "The normal (or Gaussian) distribution was discovered in 1733 by Abraham de Moivre as an approximation to the binomial distribution when the number of trails is large.    \n",
    "\n",
    "![image Abraham de Moivre \"](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Abraham_de_moivre.jpg/300px-Abraham_de_moivre.jpg)\n",
    "\n",
    "- from [Abraham de Moivre - Wikipedia)](https://en.wikipedia.org/wiki/Abraham_de_Moivre)\n",
    "\n",
    "The Gaussian distribution was derived in 1809 by Carl Friedrich Gauss.    \n",
    "\n",
    "![image Carl Friedrich Gauss \"](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Carl_Friedrich_Gauss.jpg/330px-Carl_Friedrich_Gauss.jpg)\n",
    "\n",
    "- from [Carl Friedrich Gauss - Wikipedia)](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss)\n",
    "\n",
    "Importance lies in the Central Limit Theorem, which states that the sum of a large number of independent random variables (binomial, Poisson, etc.) will approximate a normal distribution\n",
    "\n",
    "\n",
    "## Central Limit Theorem\n",
    "\n",
    "In probability theory, the central limit theorem (CLT) states that, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed, regardless of the underlying distribution. The central limit theorem has a number of variants. In its common form, the random variables must be identically distributed. \n",
    "\n",
    "- from [Central Limit Theorem - Wikipedia)](https://en.wikipedia.org/wiki/Central_limit_theorem)\n",
    "   \n",
    "\n",
    "The Central Limit Theorem tells us that when the sample size is large the average $\\bar{Y}$ of a random sample follows a normal distribution centered at the population average $\\mu_Y$ and with standard deviation equal to the population standard deviation $\\sigma_Y$, divided by the square root of the sample size $N$. \n",
    "\n",
    "This means that if we subtract a constant from a random variable, the mean of the new random variable shifts by that constant. If $X$ is a random variable with mean $\\mu$ and $a$ is a constant, the mean of $X - a$ is $\\mu-a$. \n",
    "\n",
    "This property also holds for the spread, if $X$ is a random variable with mean $\\mu$ and SD $\\sigma$, and $a$ is a constant, then the mean and SD of $aX$ are $a \\mu$ and $\\|a\\| \\sigma$ respectively.\n",
    "This implies that if we take many samples of size $N$ then the quantity \n",
    "\n",
    "$$\n",
    "\\frac{\\bar{Y} - \\mu}{\\sigma_Y/\\sqrt{N}}\n",
    "$$\n",
    "\n",
    "is approximated with a normal distribution centered at 0 and with standard deviation 1.\n",
    "\n",
    "## The t-distribution\n",
    "\n",
    "In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. Whereas a normal distribution describes a full population, t-distributions describe samples drawn from a full population; accordingly, the t-distribution for each sample size is different, and the larger the sample, the more the distribution resembles a normal distribution.\n",
    "The t-distribution plays a role in a number of widely used statistical analyses, including the Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. The Student's t-distribution also arises in the Bayesian analysis of data from a normal family.\n",
    "\n",
    "- from [The t-distribution - Wikipedia)](https://en.wikipedia.org/wiki/Student%27s_t-distribution)\n",
    "\n",
    "When the CLT does not apply (i.e. as the number of samples is large), there is another option that does not rely on large samples When a the original population from which a random variable, say $Y$, is sampled is normally distributed with mean 0 then we can calculate the distribution of \n",
    "\n",
    "\n",
    "number of variants. In its common form, the random variables must be identically distributed. \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\sqrt{N} \\frac{\\bar{Y}}{s_Y}\n",
    "$$\n",
    "\n",
    "\n",
    "![image Student's t-distribution \"](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/488px-Student_t_pdf.svg.png)\n",
    "\n",
    "Normal cumulative distribution function\n",
    "![image Normal cumulative Student's t-distribution \"](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Student_t_cdf.svg/488px-Student_t_cdf.svg.png)\n",
    "\n",
    "_Binomial Distribution_\n",
    "\n",
    "In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent yes/no experiments, each of which yields success with probability p. A success/failure experiment is also called a Bernoulli experiment or Bernoulli trial; when n = 1, the binomial distribution is a Bernoulli distribution.\n",
    "\n",
    "- from [Binomial Distribution - Wikipedia](https://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "\n",
    "\n",
    "   \n",
    "Binomial Distribution    \n",
    "![image Binomial Distribution  \"](https://upload.wikimedia.org/wikipedia/commons/thumb/7/75/Binomial_distribution_pmf.svg/450px-Binomial_distribution_pmf.svg.png)\n",
    "\n",
    "Binomial cumulative distribution function\n",
    "![image Binomial cumulative distribution function \"](https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Binomial_distribution_cdf.svg/450px-Binomial_distribution_cdf.svg.png)\n",
    "\n",
    "\n",
    "* The data arise from a sequence of n independent trials.\n",
    "* At each trial there are only two possible outcomes, conventionally called success and failure.\n",
    "* The probability of success, p, is the same in each trial.\n",
    "* The random variable of interest is the number of successes, X, in the n trials.\n",
    "* The assumptions of independence and constant p are important. If they are invalid,  so is the binomial distribution\n",
    "\n",
    "_Bernoulli Random Variables_\n",
    "\n",
    "* Imagine a simple trial with only two possible outcomes\n",
    "    * Success (S) with probabilty p.\n",
    "    * Failure (F) with probabilty 1-p.\n",
    "\n",
    "* Examples\n",
    "    * Toss of a coin (heads or tails)\n",
    "    * Gender of a newborn (male or female)\n",
    "\n",
    "## Poisson Distribution\n",
    "\n",
    "$X$ expresses the number of \"rare\" events\n",
    "\n",
    "$$\n",
    "X \\quad \\sim P( \\lambda )\\quad \\lambda \\gt 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "    P(X = x) = \\frac{ \\mathrm{e}^{- \\lambda } \\lambda^x }{x!}  \\quad x=1,2,...,n\n",
    "$$\n",
    "\n",
    "\n",
    "_Poisson Distribution_\n",
    "\n",
    "In probability theory and statistics, the Poisson distribution, named after French mathematician Siméon Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a constant rate per time unit and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\n",
    "\n",
    "For instance, an individual keeping track of the amount of mail they receive each day may notice that they receive an average number of 4 letters per day. If receiving any particular piece of mail doesn't affect the arrival times of future pieces of mail, i.e., if pieces of mail from a wide range of sources arrive independently of one another, then a reasonable assumption is that the number of pieces of mail received per day obeys a Poisson distribution. Other examples that may follow a Poisson: the number of phone calls received by a call center per hour, the number of decay events per second from a radioactive source, or the number of taxis passing a particular street corner per hour.\n",
    "\n",
    "The Poisson distribution gives us a probability mass for discrete natural numbers *k* given some mean value &lambda;. Knowing that, on average, &lambda; discrete events occur over some time period, the Poisson distribution gives us the probability of seeing exactly *k* events in that time period.\n",
    "\n",
    "For example, if a call center gets, on average, 100 customers per day, the Poisson distribution can tell us the probability of getting exactly 150 customers today.\n",
    "\n",
    "*k* &isin; **N** (i.e. is a natural number) because, on any particular day, you can't have a fraction of a phone call. The probability of any non-integer number of people calling in is zero. E.g., P(150.5) = 0.\n",
    "\n",
    "&lambda; &isin; **R** (i.e. is a real number) because, even though any *particular* day must have an integer number of people, the *mean* number of people taken over many days can be fractional (and usually is). It's why the \"average\" number of phone calls per day could be 3.5 even though half a phone call won't occur.\n",
    "\n",
    "\n",
    "- from [Poisson Distribution - Wikipedia)](https://en.wikipedia.org/wiki/Poisson_distribution)\n",
    "   \n",
    "Poisson Distribution    \n",
    "![image Poisson Distribution  \"](https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/488px-Poisson_pmf.svg.png)\n",
    "\n",
    "Poisson cumulative distribution function\n",
    "![image Poisson cumulative distribution function \"](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Poisson_cdf.svg/488px-Poisson_cdf.svg.png)\n",
    "\n",
    "\n",
    "_Properties of Poisson distribution_\n",
    "\n",
    "* The mean number of successes from n trials is µ = np\n",
    "* If we substitute µ/n for p, and let n tend to infinity, the binomial distribution becomes the Poisson distribution.\n",
    "* Poisson distributions are often used to describe the number of occurrences of a ‘rare’ event. For example\n",
    "    * The number of storms in a season\n",
    "    * The number of occasions in a season when river levels exceed a certain value\n",
    "* The main assumptions are that events occur \n",
    "    * at random (the occurrence of an event doesn’t change the probability of  it happening again) \n",
    "    *  at a constant rate \n",
    "* Poisson distributions also arise as approximations  to  binomials when n is large and p is small.\n",
    "* When there is a large number of trials, but a very small probability of success, binomial calculation becomes impractical\n",
    "\n",
    "## Deriving the Poisson Distribution from the Binomial Distribution\n",
    "\n",
    "Let’s make this a little more formal. The binomial distribution works when we have a fixed number of events n, each with a constant probability of success p. In the Poisson Distribution, we don't know the number of trials that will happen. Instead, we only know the average number of successes per time period, the rate $\\lambda$. So we know the rate of successes per day, or per minute but not the number of trials n or the probability of success p that was used to estimate to that rate. \n",
    "\n",
    "If n is the number of trails in our time period, then np is the success rate or $\\lambda$, that is, $\\lambda$ = np. Solving for p, we get:\n",
    "\n",
    "$$\n",
    "p=\\frac{\\lambda}{n} \\quad(1)\n",
    "$$\n",
    "Since the Binomial distribution is defined as below \n",
    "$$\n",
    " P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} \\quad k=1,2,...,n\n",
    "\\quad  (2)\n",
    "$$\n",
    "or equivelently\n",
    "$$\n",
    " P(X=k) = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} \\quad k=1,2,...,n\n",
    "\\quad  (3)\n",
    "$$\n",
    "By substituting the above p from (1) into the binomial distribution (3)\n",
    " $$\n",
    " P(X=k) = \\frac{n!}{k!(n-k)!} {\\frac{\\lambda!}{n}}^k (1-{\\frac{\\lambda!}{n} })^{n-k}  \\quad  (4)\n",
    " $$\n",
    "\n",
    "         \n",
    " \n",
    "For n large and p small:\n",
    " \n",
    "$$\n",
    "    P(X = k) \\equiv \\frac{ \\mathrm{e}^{- \\lambda } \\lambda^k }{k!}  \\quad k=1,2,...,n\\quad  (5)\n",
    "$$\n",
    "                                        \n",
    " \n",
    "Which is the probability mass function for the Poisson distribution.\n",
    "\n",
    "## Fat-Tailed Distribution\n",
    "\n",
    "In probability theory, the Fat-Tailed (or Gaussian) distribution is a very common continuous probability distribution. The Fat-Tailed distribution is remarkably useful because of the central limit theorem. In its most general form, under mild conditions, it states that averages of random variables independently drawn from independent distributions are Fat-Tailedly distributed. Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly Fat-Tailed. \n",
    "\n",
    "- from [Fat-Tailed Distribution - Wikipedia)](https://en.wikipedia.org/wiki/Fat-Tailed_distribution)\n",
    "   \n",
    "\n",
    "_Properties of Fat-Tailed distribution_\n",
    "\n",
    "* Power law distributions: \n",
    "    * for variables assuming integer values > 0\n",
    "    * Prob [X=k] ~ Ck-α\n",
    "    * typically 0 < alpha < 2; smaller a gives heavier tail  \n",
    "* For binomial, normal, and Poisson distributions the tail probabilities approach 0 exponentially fast \n",
    "* What kind of phenomena does this distribution model?\n",
    "* What kind of process would generate it?\n",
    "\n",
    "## Cauchy Distribution\n",
    "\n",
    "An example of a Fat-tailed distribution is the Cauchy distribution.\n",
    "\n",
    "_Cauchy Distribution_\n",
    "\n",
    " The Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution. The simplest Cauchy distribution is called the standard Cauchy distribution. It is the distribution of a random variable that is the ratio of two independent standard normal variables and has the probability density function\n",
    "\n",
    "The Cauchy distribution is often used in statistics as the canonical example of a \"pathological\" distribution since both its mean and its variance are undefined. (But see the section Explanation of undefined moments below.) The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist.[1] The Cauchy distribution has no moment generating function.\n",
    "\n",
    "- from [Cauchy Distribution - Wikipedia)](https://en.wikipedia.org/wiki/Cauchy_distribution)\n",
    "   \n",
    "Cauchy Distribution    \n",
    "![image Cauchy Distribution  \"](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Cauchy_pdf.svg/450px-Cauchy_pdf.svg.png)\n",
    "\n",
    "Cauchy cumulative distribution function\n",
    "![image Cauchy cumulative distribution function \"](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Cauchy_cdf.svg/450px-Cauchy_cdf.svg.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised Learning tries to find hidden structure in unlabeled data. Unsupervised Learning has no feedback. This typically means that the data is not labled. Supervised learning has feedback/labels.\n",
    "\n",
    "For example, if we had some e-mail that labbled some e-mail as spam and not-spam we could use a supervised learning algorthm like [Naive Bayes spam filtering](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering) to classify any new e-mail as spam or non-spam.\n",
    "\n",
    "However, if we didn't have labeled data (also called training data) we might still be able to cluster spam and non-spam if we could come up with some measure of similarity or distance between words in the e-mails.\n",
    "\n",
    "\n",
    "![Unsupervised Learning](http://nikbearbrown.com/YouTube/MachineLearning/M04/Unsupervised_Learning_A.png) \n",
    "\n",
    "*Unsupervised Learning is computational machinary to extract patterns from unlabled data*  \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is probably the most common technique whose goal is to group data into similar groups based on a similarity/distance measure. Most of the focus of this module will be on clustering. We'll discuss and implement clustering in Lesson 2 of this module\n",
    "\n",
    "#### Types of  Clustering\n",
    "\n",
    "### Partitioning-based clustering\n",
    "\n",
    "Partition-based clustering iterativley assigns data to groups based on a [disatnce metric](https://en.wikipedia.org/wiki/Metric_(mathematics)) or [similarity](https://en.wikipedia.org/wiki/Similarity_measure)/dissimilarity measures until the group assignments are stable (i.e. the algorithm converges.)\n",
    "\n",
    "![K-means](http://nikbearbrown.com/YouTube/MachineLearning/M04/Kmeans_Clustering.png) \n",
    "\n",
    "\n",
    "*K-means clustering*  \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n",
    "\n",
    "### Hierarchical clustering  \n",
    "\n",
    "[Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a method of cluster analysis which builds a hierarchy of clusters either from the \"bottom up\" (agglomerative hierarchical clustering) by combiinimg the two \"closet\" or \"most similar\" points into a cluster then the next two closet, until all points are  merged as one moves up the hierarcy.\n",
    "\n",
    "Conversely the data is split \"top down\" (divisive hierarchical clustering) so that there is the most seperation between the split groups and further splits are performed recursively untl there is a single data point in each group.\n",
    "\n",
    "![Hierarchical clustering](http://nikbearbrown.com/YouTube/MachineLearning/M04/Hierarchical_Clustering.png) \n",
    "\n",
    "*Hierarchical clustering*  \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n",
    "\n",
    "### Density-based clustering\n",
    "\n",
    "ensity-based clustering is based on probabilty distribution models. Clusters are defined as a set of distibutions then assigning points to the distibutions that they are most likely to belong. The most prominent method is known as Gaussian [mixture models](https://en.wikipedia.org/wiki/Mixture_model) that represent the clusters as multivariate normal distributions.  \n",
    "\n",
    "![Gaussian mixture model](http://nikbearbrown.com/YouTube/MachineLearning/M04/Gaussian_Mixture_Model.png) \n",
    "\n",
    "$$ p(\\boldsymbol{\\theta}) = \\sum_{i=1}^K\\phi_i \\mathcal{N}(\\boldsymbol{\\mu_i,\\sigma_i}) $$\n",
    "\n",
    "*Gaussian mixture model*  \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n",
    "\n",
    "## Expectation-maximization\n",
    "\n",
    "The [Expectation-maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is an iterative method for finding [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood) or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. We'll discuss and implement association rule learning in Lesson 4 of this module.\n",
    "\n",
    "Given a statistica which generates a set $\\mathbf{X}$ of observed data, a set of unobserved latent data or missing values $\\mathbf{Z}$, and a vector of unknown parameters $\\boldsymbol\\theta$, along with a likelihood function $L(\\boldsymbol\\theta; \\mathbf{X}$, $\\mathbf{Z}) = p(\\mathbf{X}$, $\\mathbf{Z}|\\boldsymbol\\theta)$, the maximum likelihood estimate (MLE) of the unknown parameters is determined by the marginal likelihood of the observed data.  \n",
    "\n",
    "$$ L(\\boldsymbol\\theta; \\mathbf{X}) = p(\\mathbf{X}|\\boldsymbol\\theta) = \\sum_{\\mathbf{Z}} p(\\mathbf{X},\\mathbf{Z}|\\boldsymbol\\theta) $$ \n",
    " \n",
    "However, this quantity is often intractable (e.g. if $\\mathbf{Z}$ is a sequence of events, so that the number of values grows exponentially with the sequence length, making the exact calculation of the sum extremely difficult).  \n",
    "\n",
    "The EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying the following two steps:  \n",
    "\n",
    "*Expectation step (E step):* Calculate the expected value of the log likelihood function, with respect to the conditional distribution of $\\mathbf{Z}$ given $\\mathbf{X}$ under the current estimate of the parameters\n",
    " $$ \n",
    "\\boldsymbol\\theta^{(t)}:\n",
    "Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)}) = \\operatorname{E}_{\\mathbf{Z}|\\mathbf{X},\\boldsymbol\\theta^{(t)}}\\left[ \\log L (\\boldsymbol\\theta;\\mathbf{X},\\mathbf{Z})  \\right]  $$ \n",
    "\n",
    "*Maximization step (M step):* Find the parameter that maximizes this quantity:\n",
    " $$ \n",
    "\\boldsymbol\\theta^{(t+1)} = \\underset{\\boldsymbol\\theta}{\\operatorname{arg\\,max}} \\ Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)}) $$ \n",
    "\n",
    "\n",
    "\n",
    "## Linear Discriminant Analysis (LDA)  \n",
    "\n",
    "[Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) which is related to blind signal separation techniques, such as those we've already studied in the dimensionality reduction module like: Principal component analysis, Independent component analysis,Non-negative matrix factorization, and  Singular value decomposition. LDA explicitly attempts to model the difference between the classes of data. That is,  like regression analysis, LDA attempts to express one response variable as a linear combination of other features or measurements. So while the math is similar to PCA and SVD, the intent is similar to regression (especially logistic regression) in that it creates a model. We'll discuss and implement association rule learning in Lesson 4 of this module.\n",
    "\n",
    "LDA is often based upon s Fisher's linear discriminant. Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:\n",
    "\n",
    "$$ S=\\frac{\\sigma_{\\text{between}}^2}{\\sigma_{\\text{within}}^2}= \\frac{(\\vec w \\cdot \\vec \\mu_1 - \\vec w \\cdot \\vec \\mu_0)^2}{\\vec w^T \\sigma_1 \\vec w + \\vec w^T \\sigma_0 \\vec w} = \\frac{(\\vec w \\cdot (\\vec \\mu_1 - \\vec \\mu_0))^2}{\\vec w^T (\\sigma_0+\\sigma_1) \\vec w} $$\n",
    "\n",
    "This measure is, in some sense, a measure of the signal-to-noise ratio for the class labelling. It can be shown that the maximum separation occurs when\n",
    "\n",
    "$$  \\vec{w} \\propto (\\sigma_0+\\sigma_1)^{-1}(\\vec{\\mu}_1 - \\vec{\\mu}_0) $$ \n",
    "When the assumptions of LDA are satisfied, the above equation is equivalent to LDA.\n",
    "\n",
    "In essence, LDA picks a new basis that gives:\n",
    "\n",
    "\n",
    "To do this, LDA uses eigenvectors based on between-class and  within-class covariance matrices.\n",
    "\n",
    "$$ max\\frac{(\\mu_1 - \\mu_2)^2}{\\sigma_1^2 + \\sigma_2^2}  $$\n",
    "\n",
    "![Linear Discriminant Analysis (LDA)](http://nikbearbrown.com/YouTube/MachineLearning/M04/Linear_Discriminant_Analysis_LDA.png) \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n",
    "## Association rule learning  \n",
    "\n",
    "[Association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) is an Unsupervised alogorthm interesting relations between variables in a data set. We'll discuss and implement association rule learning in Lesson 5 of this module. Association rule learning aims to discover interesting correlation or other relationships in large databases. It finds a rule of the form:\n",
    "\n",
    "$$ if\\quad A \\quad and \\quad B \\quad then \\quad C \\quad and \\quad D $$ \n",
    "\n",
    "![Association rule Set Enumeration Tree](http://nikbearbrown.com/YouTube/MachineLearning/M04/Assoc_Rule_Set_Enumeration_Tree.png)  \n",
    "\n",
    "*Association rule Set Enumeration Tree*  \n",
    "\n",
    "## Artificial neural networks (ANN)\n",
    "\n",
    "[Artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are a  are a family of statistical learning models inspired by biological neural networks. Some neural network models like \n",
    "[Self-organizing map's](https://en.wikipedia.org/wiki/Self-organizing_map),[adaptive resonance theory (ART)](https://en.wikipedia.org/wiki/Adaptive_resonance_theory), [Restricted Boltzmann machines](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine) and [Hopfield network's](https://en.wikipedia.org/wiki/Hopfield_network). We'll discuss and implement various neural statistical learning models in a module entirely devoted to neural networks. We'll simply note here that some statistical neural learning models are unsupervised and put off discussing the details until the neural network module.\n",
    "\n",
    "![An illustration of the training of a self-organizing map](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Somtraining.svg/1000px-Somtraining.svg.png)  \n",
    "\n",
    "An illustration of the training of a self-organizing map  \n",
    "\n",
    "Art [\"Somtraining\"](https://commons.wikimedia.org/wiki/File:Somtraining.svg##/media/File:Somtraining.svg) by [Mcld](http://www.mcld.co.uk/). Licensed under [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0) \n",
    "\n",
    "\n",
    "![Hopfield network](http://nikbearbrown.com/YouTube/MachineLearning/M04/Hopfield_Network.png)  \n",
    "\n",
    "*An illustration of a stable Hopfield network*   \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n",
    "\n",
    "## Big Data and Unsupervised Learning\n",
    "\n",
    "“Every day, we create 2.5 quintillion bytes of data—so much that 90% of the data in the world today has been created in the last two years alone.” $[1]$ Due to the growing complexity of digital social networks and the huge quantity of data they produce daily, it’s important that we deal with “big-data” efficiently. A number of tools and technologies (Map-Reduce, NoSQL, Hadoop, Hive, cloud computing, parallel processing, clustering, MPP, virtualization, large grid environments, and so on) have been developed to store and process big data. While big-data technologies have established the ability to collect and process large amounts of data, most organizations struggle with understanding the data and taking advantage of its value. According to an Economist report: “Extracting value from big data remains elusive for many organizations. For most companies today, data are abundant and readily available, but not well used.”$[2,3]$\n",
    "\n",
    "A central issue with “big data” is that not all of the data will labled. As such, there is an expectation that unsupervised learning and research in unsupervised learning will take on an even greater importance in machine learning.  \n",
    "\n",
    "\n",
    "![big data](http://nikbearbrown.com/YouTube/MachineLearning/M04/Big_Data_Twitter.png)  \n",
    "\n",
    "Image courtesy of [clipartist.net](http://clipartist.net/)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering\n",
    "\n",
    "What is [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis)?\n",
    "Clustering is grouping like with like such that:  \n",
    "\n",
    "* Similar objects are close to one another within the same cluster.  \n",
    "* Dissimilar to the objects in other clusters.  \n",
    "\n",
    "![cluster analysis](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/440px-Cluster-2.svg.png)\n",
    "\n",
    "*cluster analysis*  \n",
    "  \n",
    "# Distance & Similarity Measures\n",
    "\n",
    "There are two primary approaches to measure the \"closeness\" of data, distance and similarity. Distance measures are based in the notion of a [metric space](https://en.wikipedia.org/wiki/Metric_space).\n",
    "\n",
    "[Similarity measures](https://en.wikipedia.org/wiki/Similarity_measure) s a real-valued function that quantifies the similarity between two objects. While no single definition of a similarity measure exists, they don't necessarily have the constraint of being a metric space.  For example, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is a measure of the \"closeness\" of two vectors that is not a metric space.\n",
    "\n",
    "## Metric Spaces\n",
    "\n",
    "A *metric space* is an *ordered pair* $(M,d)$ where $M$ is a set and $d$ is a distance (i.e.metric) on $M$, i.e., a function:  \n",
    "\n",
    "$d \\colon M \\times M \\rightarrow \\mathbb{R}$\n",
    "\n",
    "such that for any $x, y, z \\in M$, the following holds:  \n",
    "\n",
    "* $d(x,y) \\ge 0 \\quad (non-negative),$  \n",
    "* $d(x,y) = 0, \\iff x = y \\quad (identity \\quad of \\quad indiscernibles),$  \n",
    "* $d(x,y) = d(y,x), \\quad (symmetry).$  \n",
    "* $d(x,z) \\le d(x,y) + d(y,z) \\quad  (triangle \\quad inequality)$  \n",
    "\n",
    "### Examples of Metric Spaces\n",
    "\n",
    "Examples of Metric Spaces are [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance), [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance), [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) and many others.\n",
    " \n",
    "#### Euclidean distance\n",
    "\n",
    "Euclidean distance is the most common metric for measuring the distance between two vectors. The is the stadard Cartesian coordinates. That is, if $p = (p_1, p_2,..., p_n)$ and $q = (q_1, q_2,..., q_n)$ are two points in Euclidean n-space, then the distance (d) from p to q, or from q to p is given by the Pythagorean formula:\n",
    "\n",
    "$$\\begin{align}\\mathrm{d}(\\mathbf{p},\\mathbf{q}) = \\mathrm{d}(\\mathbf{q},\\mathbf{p}) & = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2} \\\\[8pt]\n",
    "& = \\sqrt{\\sum_{i=1}^n (q_i-p_i)^2}.\\end{align}$$  \n",
    "\n",
    "\n",
    "## Similarity Measures\n",
    "\n",
    "Similarity (or dimilarity measures) measure closeness without the constraints and benefits of being a formal metric space.\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "The most common form of the similarity measure is the vector inner product (or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity))\n",
    "Given vectors A and B, the vector inner product can be defined using the [Euclidean dot product](https://en.wikipedia.org/wiki/Euclidean_vector#Dot_product) formula:\n",
    "\n",
    "$$\\mathbf{a}\\cdot\\mathbf{b}\n",
    "=\\left\\|\\mathbf{a}\\right\\|\\left\\|\\mathbf{b}\\right\\|\\cos\\theta$$\n",
    "\n",
    "This similarity measure can also be ranged normalized. Alternately, we can normalize this measure by dividing each vector component by the magnitude of the vector.\n",
    "\n",
    "### Pearson correlation\n",
    "\n",
    "Correlation based similarity is usually the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient). Pearson product-moment correlation coefficient commonly represented by the Greek letter $\\rho$ (rho) and is defined as:\n",
    "\n",
    "$$ \\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} $$\n",
    "\n",
    "where $\\operatorname{cov}$ is the [covariance](https://en.wikipedia.org/wiki/Covariance) and  $\\sigma_X$ is the [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) of $X$.  \n",
    "\n",
    "The formula for $\\rho$can be expressed in terms of mean and expectation.  \n",
    "\n",
    "$$\\operatorname{cov}(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)]$$ \n",
    "\n",
    "\n",
    "# Clustering Data Structures\n",
    "\n",
    "* Data matrix\n",
    "\n",
    "$$ \n",
    "M=\n",
    "  \\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & x_{1,3} & ...  & x_{1,n} \\\\\n",
    "    x_{2,1} & x_{2,2} & x_{2,3} & ...  & x_{2,n} \\\\\n",
    "    x_{3,1} & x_{3,2} & x_{3,3} & ...  & x_{3,n} \\\\    \n",
    "    .. & .. & .. & ...  & .. \\\\    \n",
    "    x_{n,1} & x_{n,2} & x_{n,3} & ...  & x_{n,n} \\\\    \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Distance matrix\n",
    "\n",
    "$$ \n",
    "M=\n",
    "  \\begin{bmatrix}\n",
    "    x_{1,1} &   &  &   &  \\\\   \n",
    "    x_{2,1} & 0  &  &   &  \\\\   \n",
    "    x_{3,1} & x_{3,2} & 0 &   &  \\\\    \n",
    "    .. & .. & .. & ...  & .. \\\\    \n",
    "    x_{n,1} & x_{n,2} & x_{n,3} & ...  & 0 \\\\    \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Dissimilarity/Similarity matrix\n",
    "\n",
    "\n",
    "The dissimilarity/similarity matrix is calculated by iterating over each element and calculating its dissimilarity/similarity to every other element. Let A be a Dissimilarity Matrix of size $NxN$, and $B$ a set of $N$ elements. $A_{i,j}$ is the dissimilarity/similarity between elements $B_i$ and $B_j$.  \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "   for i = 0 to N do\n",
    "       for j = 0 to N do\n",
    "           Aij = Dissimilarity(Bi,Bj) // or Similarity(Bi,Bj)\n",
    "       end-for\n",
    "   end-for\n",
    "   \n",
    "```     \n",
    "   \n",
    "where the dissimilarity/similarity matrix is usually defined as follows:\n",
    "\n",
    "$$ \n",
    "M=\n",
    "  \\begin{bmatrix}\n",
    "    0 &   &  &   &  \\\\   \n",
    "   d(2,1)  & 0  &  &   &  \\\\   \n",
    "   d(3,1)  & d(3,2) & 0 &   &  \\\\    \n",
    "    .. & .. & .. & ...  & .. \\\\    \n",
    "    d(n,1)  & d(n,2) & d(n,3) & ...  & 0 \\\\    \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "# Types of  Clustering\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) (e.g., k-means, mixture models, hierarchical clustering). Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). There are various types of cluster analysis.  \n",
    "\n",
    "* Partitioning-based clustering (K-means and its variants)\n",
    "* Hierarchical clustering\n",
    "* Density-based clustering\n",
    "\n",
    "## Partitioning-based clustering (K-means and its variants)\n",
    "\n",
    "Partitioning algorithms: Construct various partitions and then evaluate them by some criterion\n",
    "Hierarchy algorithms: Create a hierarchical decomposition of the set of data (or objects) using some criterion\n",
    "Density-based: based on connectivity and density functions\n",
    "Grid-based: based on a multiple-level granularity structure\n",
    "Model-based: A model is hypothesized for each of the clusters and the idea is to find the best fit of that model to each other\n",
    "\n",
    "\n",
    "Partitioning method: Construct a partition of n documents into a set of K clusters\n",
    "Given: a set of documents and the number K \n",
    "Find: a partition of K clusters that optimizes the chosen partitioning criterion\n",
    "Globally optimal\n",
    "Intractable for many objective functions\n",
    "Ergo, exhaustively enumerate all partitions\n",
    "Effective heuristic methods: K-means and K-medoids algorithms\n",
    "\n",
    "\n",
    "### K-means\n",
    "\n",
    "The term \"k-means\" was first used by James MacQueen in 1967,[1] though the idea goes back to Hugo Steinhaus in 1957. Given a desired $k$ clusters and n data points $X = {x_1, x_2, …, x_n}$: \n",
    "\n",
    "Initialize centroids $\\mu_1, \\mu_1, ... \\mu_k \\quad \\in \\quad  \\mathbb{R}^n$  (usually randomly)  \n",
    "\n",
    "\n",
    "while (not coverged):\n",
    "\n",
    "Step A (Assignment step):  \n",
    "\n",
    "  Find the closest cluster to every point in $X = {x_1, x_2, …, x_n}$\n",
    "  \n",
    "  That is,   \n",
    "$$\\underset{\\mathbf{X}} {\\operatorname{arg\\,min}}  \\sum_{i=1}^{k} \\sum_{\\mathbf x \\in X_i} \\left\\| \\mathbf x - \\boldsymbol\\mu_i \\right\\|^2$$\n",
    "\n",
    "where $\\mu_i$ is the mean of points in $X_i$.\n",
    "\n",
    "$$\n",
    "S_i^{(t)} = \\big \\{ x_p : \\big \\| x_p - m^{(t)}_i \\big \\|^2 \\le \\big \\| x_p - m^{(t)}_j \\big \\|^2 \\ \\forall j, 1 \\le j \\le k \\big\\}\n",
    "$$  \n",
    "\n",
    "Step B (Update step): \n",
    "\n",
    "  Calculate the new means to be the centroids of the observations in the new clusters.\n",
    "\n",
    "$$\n",
    "m^{(t+1)}_i = \\frac{1}{|X^{(t)}_i|} \\sum_{x_j \\in X^{(t)}_i} x_j \n",
    "$$\n",
    "\n",
    "The [centroid](https://en.wikipedia.org/wiki/Centroid) of a finite set of {k} points  \n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_k in \\mathbb{R}^n \\quad is\\quad \n",
    "\\mathbf{C} = \\frac{\\mathbf{x}_1+\\mathbf{x}_2+\\cdots+\\mathbf{x}_k}{k}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is the machine learning task of inferring a function from labeled training data.[1] The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bias–variance tradeoff\n",
    "\n",
    "The [Bias–variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\n",
    "\n",
    "The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "\n",
    "The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\n",
    "\n",
    "The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\n",
    "This tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning.\n",
    "\n",
    "This is the essence of the **bias-variance tradeoff**: You are seeking a model that appropriately balances bias and variance, and thus will generalize to new data (known as \"out-of-sample\" data).\n",
    "\n",
    "### Overfitting \n",
    "\n",
    "**What is overfitting?**\n",
    "\n",
    "- Building a model that matches the training data \"too closely\"\n",
    "- Learning from the noise in the data, rather than just the signal\n",
    "\n",
    "**How does overfitting occur?**\n",
    "\n",
    "- Evaluating a model by testing it on the same data that was used to train it\n",
    "- Creating a model that is \"too complex\"\n",
    "\n",
    "**What is the impact of overfitting?**\n",
    "\n",
    "- Model will do well on the training data, but won't generalize to out-of-sample data\n",
    "- Model will have low bias, but high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Models and Regression\n",
    "\n",
    "Linear regression predicts the response variable $y$ assuming it has a linear relationship with predictor variable(s) $x$ or $x_1, x_2, ,,, x_n$.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\varepsilon .$$\n",
    "\n",
    "*_Simple regression_* use only one predictor variable $x$. *_Mulitple regression_* uses a set of predictor variables $x_1, x_2, ,,, x_n$.\n",
    "\n",
    "The *response variable* $y$ is also called the regressand, forecast, dependent or explained variable. The *predictor variable* $x$ is also called the regressor, independent or explanatory variable.\n",
    "\n",
    "The parameters $\\beta_0$ and $\\beta_1$ determine the intercept and the slope of the line respectively. The intercept $\\beta_0$ represents the predicted value of $y$ when $x=0$. The slope $\\beta_1$ represents the predicted increase in $Y$ resulting from a one unit increase in $x$.\n",
    "\n",
    "Note that the regression equation is just our famliar equation for a line with an error term.\n",
    "\n",
    "The equation for a line:  \n",
    "$$ Y = bX + a $$\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "The equation for a line with an error term:  \n",
    "\n",
    "$$ Y = bX + a + \\varepsilon $$\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\varepsilon .$$\n",
    "\n",
    "- $b$ = $\\beta_1$ = slope\n",
    "- $a$ = $\\beta_0$ = $Y$ intercept\n",
    "- $\\varepsilon$ = error term\n",
    "\n",
    "\n",
    "We can think of each observation $y_i$ consisting of the systematic or explained part of the model, $\\beta_0+\\beta_1x_i$, and the random *error*, $\\varepsilon_i$.\n",
    "\n",
    "_Zero Slope_\n",
    "\n",
    "Note that when  $\\beta_1 = 0$ then response does not change as the predictor changes.\n",
    "\n",
    "\n",
    "## The error $\\varepsilon_i$\n",
    "\n",
    "The error term is a catch-all for anything that may affect $y_i$ other than $x_i$. We assume that these errors:\n",
    "\n",
    "* have mean zero; otherwise the forecasts will be systematically biased.\n",
    "* statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data).\n",
    "* homoscedasticity (constant variance) of the errors.\n",
    "* normality of the error distribution.\n",
    "\n",
    "If any of these assumptions is violated then the robustness of the model to be taken with a grain of salt.\n",
    "\n",
    "\n",
    "## Least squares estimation\n",
    "\n",
    "In a linear model, the values of $\\beta_0$ and $\\beta_1$. These need to be estimated from the data. We call this *fitting a model*.\n",
    "\n",
    "The least squares method iis the most common way of estimating $\\beta_0$ and $\\beta_1$ by minimizing the sum of the squared errors. The values of $\\beta_0$ and $\\beta_1$ are chosen so that that minimize\n",
    "\n",
    "$$\\sum_{i=1}^N \\varepsilon_i^2 = \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1x_i)^2. $$\n",
    "\n",
    "\n",
    "Using mathematical calculus, it can be shown that the resulting **least squares estimators** are\n",
    "\n",
    "$$\\hat{\\beta}_1=\\frac{ \\sum_{i=1}^{N}(y_i-\\bar{y})(x_i-\\bar{x})}{\\sum_{i=1}^{N}(x_i-\\bar{x})^2} $$ \n",
    "\n",
    "and\n",
    "\n",
    "$$\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x}, $$\n",
    "\n",
    "where $\\bar{x}$ is the average of the $x$ observations and $\\bar{y}$ is the average of the $y$ observations. The estimated line is known as the *regression line*.\n",
    "\n",
    "\n",
    "## Fitted values and residuals\n",
    "\n",
    "The response values of $y$ obtained from the observed $x$ values are\n",
    "called *fitted values*: $\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i$, for\n",
    "$i=1,\\dots,N$. Each $\\hat{y}_i$ is the point on the regression\n",
    "line corresponding to $x_i$.\n",
    "\n",
    "The difference between the observed $y$ values and the corresponding fitted values are the *residuals*:\n",
    "\n",
    "$$e_i = y_i - \\hat{y}_i = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1x_i. $$\n",
    "\n",
    "The residuals have some useful properties including the following two:\n",
    "\n",
    "$$\\sum_{i=1}^{N}{e_i}=0 \\quad\\text{and}\\quad \\sum_{i=1}^{N}{x_ie_i}=0. $$\n",
    "\n",
    "\n",
    "Residuals are the errors that we cannot predict.Residuals are highly useful for studying whether a given regression model is an appropriate statistical technique for analyzing the relationship.\n",
    "\n",
    "## Regression and correlation\n",
    "\n",
    "The correlation coefficient $r$ measures the strength and the direction of the linear relationship between the two variables. The stronger the linear relationship, the closer the observed data points will cluster around a straight line.\n",
    "\n",
    "The _Pearson product-moment correlation coefficient_ is the most widely used of all correlation coefficients. In statistics, the Pearson product-moment correlation coefficient (/ˈpɪərsɨn/) (sometimes referred to as the PPMCC or PCC or Pearson's r) is a measure of the linear correlation (dependence) between two variables X and Y, giving a value between +1 and −1 inclusive, where 1 is total positive correlation, 0 is no correlation, and −1 is total negative correlation. It is widely used in the sciences as a measure of the degree of linear dependence between two variables. It was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s. Early work on the distribution of the sample correlation coefficient was carried out by Anil Kumar Gain and R. A. Fisher from the University of Cambridge.\n",
    "\n",
    "from [Pearson product-moment correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)  \n",
    "\n",
    "Examples of scatter diagrams with different values of correlation coefficient (ρ)\n",
    "\n",
    "The value of r is such that -1 < r < +1.  \n",
    "\n",
    "Strong positive correlation r is close to +1. Strong negative correlation r is close to -1. No correlation r is close to 0.   \n",
    "\n",
    "The advantage of a regression model over correlation is that it asserts a predictive relationship between the two variables ($x$ predicts $y$) and quantifies this in a useful way for forecasting.\n",
    "\n",
    "## The t-statistic and the standard error\n",
    "\n",
    "The t-statistic is the coefficient divided by its standard error. For example, a $\\beta_1$ of 38.2 divided by a standard error of 3.4 would give a t value of 11.2.\n",
    "\n",
    "The standard error is an estimate of the standard deviation of the coefficient. It can be thought of as the spread between fitted and actual values.\n",
    "\n",
    "$$s_e=\\sqrt{\\frac{1}{N-2}\\sum_{i=1}^{N}{e_i^2}}.$$\n",
    "\n",
    "\n",
    "If a coefficient is large compared to its standard error, then we can reject the hypothesis that $\\beta_1$ = 0. Intuitively, we can think of this if the slope is not small and there is a not much spread between fitted and actual values then we can be confident that the true slope $\\hat{\\beta}_1$ is not 0.\n",
    "\n",
    "A t-statistic (t value) of greater than 2 in magnitude, corresponds to p-values less than 0.05.\n",
    "\n",
    "The p-value is a function of the observed sample results (a statistic) that is used for testing a statistical hypothesis. Before the test is performed, a threshold value is chosen, called the significance level of the test, traditionally 5% or 1% and denoted as $\\alpha$.  \n",
    "\n",
    "If the p-value is equal to or smaller than the significance level ($\\alpha$), it suggests that the observed data are inconsistent with the assumption that the null hypothesis is true and thus that hypothesis must be rejected (but this does not automatically mean the alternative hypothesis can be accepted as true). When the p-value is calculated correctly, such a test is guaranteed to control the Type I error rate to be no greater than $\\alpha$.\n",
    "\n",
    "from [P-value](https://en.wikipedia.org/wiki/P-value)\n",
    "\n",
    "## R-squared $R^2$ \n",
    "\n",
    "[R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination)   (coefficient of determination) is a statistical measure of how close the data are to the fitted regression line. the coefficient of determination, denoted $R^2$ or $r^2$ and pronounced \"R squared\", is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "R-squared = Explained variation / Total variation\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "* 0% (or 0) indicates that the model explains none of the variability of the response data around its mean.  \n",
    "* 100% (or 1) indicates that the model explains all the variability of the response data around its mean. \n",
    "\n",
    "The higher the R-squared, the better the model fits your data. \n",
    "\n",
    "The better the linear regression fits the data in comparison to the simple average (on the left graph), the closer the value of $R^2$ is to 1. The areas of the blue squares represent the squared residuals with respect to the linear regression. The areas of the red squares represent the squared residuals with respect to the average value.]]\n",
    "\n",
    "A data set has $n$ values marked $y_1$,...,$y_n$ (collectively known as $y_i$ or as a vector $y = [y_1,..., y_n]^T$), each associated with a predicted (or modeled) value $\\hat{y}_{1},...,\\hat{y}_{n}$.\n",
    "\n",
    "Define the residuals as as $e_i = y_i − \\hat{y}_{i}$ (forming a vector $e$).\n",
    "\n",
    "If $\\bar{y}$ is the mean of the observed data:\n",
    "\n",
    "$$\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i $$\n",
    "\n",
    "then the variability of the data set can be measured using three sums of square formulas:\n",
    "\n",
    "* The total sum of squares (proportional to the variance of the data):\n",
    "\n",
    " $$SS_\\text{tot}=\\sum_i (y_i-\\bar{y})^2,$$\n",
    "\n",
    "* The regression sum of squares, also called the explained sum of squares:\n",
    "\n",
    "$$SS_\\text{reg}=\\sum_i (\\hat{y}_{i} -\\bar{y})^2,$$\n",
    "\n",
    "* The sum of squares of residuals, also called the residual sum of squares:\n",
    "\n",
    "$$SS_\\text{res}=\\sum_i (y_i - \\hat{y}_{i})^2=\\sum_i e_i^2\\,$$\n",
    "\n",
    "The most general definition of the coefficient of determination is\n",
    "\n",
    "$$R^2 \\equiv 1 - {SS_{\\rm res}\\over SS_{\\rm tot}}.\\,$$\n",
    "\n",
    "$R^2$ can be thought of as *the proportion of variation in the forecast variable that is accounted for (or explained) by the regression model*.\n",
    "\n",
    "In the definition of $R^2$, $0 \\geq R^2 \\geq 1$ as is similar to he value of $r^2$ (the square of the pearson correlation between $f(x)$ and $x$.\n",
    "\n",
    "### Adjusted $R^2$  \n",
    "\n",
    "\n",
    "The use of an adjusted $R^2$ (also called  $$\\bar R^2 $$, pronounced \"R bar squared\" or another is  $$R^2_{\\text{adj}} $$) is an attempt to take account of the phenomenon of the $R^2$   automatically and spuriously increasing when extra explanatory variables are added to the model. It is a modification due to Theil of $R^2$   that adjusts for the number of explanatory variables in a model relative to the number of data points. The adjusted $R^2$   can be negative, and its value will always be less than or equal to that of $R^2$  . Unlike $R^2$  , the adjusted $R^2$   increases only when the increase in $R^2$ (due to the inclusion of a new explanatory variable) is more than one would expect to see by chance. If a set of explanatory variables with a predetermined hierarchy of importance are introduced into a regression one at a time, with the adjusted $R^2$   computed each time, the level at which adjusted $R^2$   reaches a maximum, and decreases afterward, would be the regression with the ideal combination of having the best fit without excess/unnecessary terms. The adjusted $R^2$   is defined as\n",
    "\n",
    "$$\\bar R^2 = {1-(1-R^2){n-1 \\over n-p-1}} = {R^2-(1-R^2){p \\over n-p-1}} $$\n",
    "\n",
    "where ''p'' is the total number of explanatory variables in the model (not including the constant term), and ''n'' is the sample size.\n",
    "\n",
    "Adjusted $R^2$   can also be written as\n",
    "\n",
    "$$\\bar R^2 = {1-{SS_\\text{res}/\\text{df}_e \\over SS_\\text{tot}/\\text{df}_t}} $$\n",
    "\n",
    "where $df_{t}$ is the degrees of freedom ''n''– 1 of the estimate of the population variance of the dependent variable, and $df_{e}$ is the degrees of freedom ''n'' – ''p'' – 1 of the estimate of the underlying population error variance.\n",
    "\n",
    "The principle behind the adjusted $R^2$   statistic can be seen by rewriting the ordinary $R^2$   as\n",
    "\n",
    "$$R^{2} = {1-{\\textit{VAR}_\\text{res} \\over \\textit{VAR}_\\text{tot}}} $$\n",
    "\n",
    "where  $$\\text{VAR}_\\text{res} = SS_\\text{res}/n $$ and  $$\\text{VAR}_\\text{tot} = SS_\\text{tot}/n $$ are the sample variances of the estimated residuals and the dependent variable respectively, which can be seen as biased estimates of the population variances of the errors and of the dependent variable. These estimates are replaced by statistically unbiased versions: \n",
    "\n",
    "$$\\text{VAR}_\\text{res} = SS_\\text{res}/(n-p-1) $$ and  $$\\text{VAR}_\\text{tot} = SS_\\text{tot}/(n-1) $$.\n",
    "\n",
    "Adjusted $R^2$   does not have the same interpretation as $R^2$  —while $R^2$   is a measure of fit, adjusted $R^2$   is instead a comparative measure of suitability of alternative nested sets of explanators.  As such, care must be taken in interpreting and reporting this statistic. Adjusted $R^2$   is particularly useful in the feature selection stage of model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction is about converting data of high dimensionality into data of lower dimensionality while keeping most of the information in the data. This allows us to work on larger datasets and identify the data’s most relevant features. Anomaly detection (or outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset.  In this first lesson, we study the theory of dimensionality reduction and introduce essential concepts and jargon, such as eigenvalues, eigenvectors, linear independence, span, vector, scalar, basis of a subspace and linear combination.   \n",
    "\n",
    "\n",
    "# Dimensionality Reduction\n",
    "\n",
    "In machine learning and statistics, [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) or dimension reduction is the process of reducing the number of random variables under consideration, and can be divided into feature selection and feature extraction.\n",
    "\n",
    "*Feature selection*  \n",
    "\n",
    "Feature selection approaches try to find a subset of the original variables (also called features or attributes). In essence, either using domain knowledge and statisitcal tests to prune away some of the original variables\n",
    "\n",
    "*Feature extraction*  \n",
    "\n",
    "Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions.  \n",
    "\n",
    "\n",
    "# Factor Analysis \n",
    "\n",
    "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  The observed variables are modelled as linear combinations of the potential factors, plus \"error\" terms. \n",
    "\n",
    "Factor Analysis has was first developed in 1901 by [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson). Pearson posed a model having one factor that was common across his data:\n",
    "\n",
    "$$\n",
    "Y_ij = \\alpha_i W_1 + \\varepsilon_{ij}\n",
    "$$\n",
    "\n",
    "Or a general multi-factor factor:\n",
    "\n",
    "$$\n",
    "Y_{ij} = \\sum_{k=1}^K \\alpha_{i,k} W_{j,k} + \\varepsilon_{ij}\n",
    "$$\n",
    "\n",
    "We can use various techniques to estimate $\\mathbf{W}_1,\\dots,\\mathbf{W}_K$. Choosing $k$ (the number of factors) is a challenge that we'll discuss in further sections.\n",
    "\n",
    "To illustrate the idea of feature extraction using factors consider the problem of reducing 2D data to 1D.\n",
    "\n",
    "\n",
    "![image 2D data to 1D A](http://nikbearbrown.com/YouTube/MachineLearning/M03/Eigin_Projection_A.png)     \n",
    "\n",
    "![image 2D data to 1D B](http://nikbearbrown.com/YouTube/MachineLearning/M03/Eigin_Projection_B.png)   \n",
    "\n",
    "# Linear Algebra\n",
    "\n",
    "So how do we find a good basis to project some data?\n",
    "\n",
    "## Some Linear Algebra Jargon\n",
    "\n",
    "### Real coordinate spaces  \n",
    "\n",
    "$$\\mathbb{R}^2 , \\quad \\mathbb{R}^3, ... , \\quad \\mathbb{R}^N$$\n",
    "\n",
    "### What is a vector?\n",
    "\n",
    "* A vector is a quantity having direction as well as magnitude.\n",
    "* An element of the real coordinate space $\\mathbb{R}^N$\n",
    "\n",
    "### Systems of Linear Equations\n",
    "\n",
    "Linear algebra are used to solve systems of linear equations such as this:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a + b + c &= 5\\\\\n",
    "3a - 2b + c &= 3\\\\\n",
    "2a + b  - c &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can rewrite and solve this system using matrix algebra notation:\n",
    "\n",
    "$$\n",
    "\\,\n",
    "\\begin{pmatrix}\n",
    "1&1&1\\\\\n",
    "3&-2&1\\\\\n",
    "2&1&-1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "5\\\\\n",
    "3\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\implies\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "1&1&1\\\\\n",
    "3&-2&1\\\\\n",
    "2&1&-1\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "5\\\\\n",
    "3\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Multiplying by Vector or Matrix by a Scalar\n",
    "\n",
    "scalar multiplication of a real Euclidean vector by a positive real number multiplies the magnitude of the vector without changing its direction.\n",
    "\n",
    "![image Scalar multiplication A](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Scalar_multiplication_by_r%3D3.svg/500px-Scalar_multiplication_by_r%3D3.svg.png)     \n",
    "\n",
    "\n",
    "Multiplying by a negative value changes its direction.\n",
    "\n",
    "\n",
    "![image Scalar multiplication A](https://upload.wikimedia.org/wikipedia/en/thumb/1/1b/Scalar_multiplication_of_vectors2.svg/500px-Scalar_multiplication_of_vectors2.svg.png)     \n",
    "  \n",
    "\n",
    "If $a$ is scalar and $\\mathbf{X}$ is a matrix then:\n",
    "\n",
    "$$\n",
    "a \\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    "a x_{1,1} & \\dots & a x_{1,p}\\\\\n",
    "& \\vdots & \\\\\n",
    "a x_{N,1} & \\dots & a  x_{N,p}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Properties of Scalar Multiplication  \n",
    "\n",
    "Scalar multiplication obeys the following rules:  \n",
    "* Additivity in the scalar: $(c + d)\\vec{v} = c\\vec{v} + d\\vec{v};$  \n",
    "* Additivity in the vector: $c(\\vec{v} + \\vec{w}) = c\\vec{v} + c\\vec{w};$  \n",
    "* Compatibility of product of scalars with scalar multiplication: $(cd)\\vec{v} = c(d\\vec{v});$  \n",
    "* Multiplying by 1 does not change a vector: $1\\vec{v} = \\vec{v};$  \n",
    "* Multiplying by 0 gives the zero vector: $0\\vec{v} = 0;$  \n",
    "* Multiplying by −1 gives the additive inverse: $(−1)\\vec{v} = −\\vec{v}.$  \n",
    "\n",
    "### The Matrix Transpose\n",
    "\n",
    "The matrix transpose is an operation that changes columns to rows. We use either a $\\top$ to denote transpose.   \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "x_{1,1}&\\dots & x_{1,p} \\\\\n",
    "x_{2,1}&\\dots & x_{2,p} \\\\\n",
    "& \\vdots & \\\\\n",
    "x_{N,1}&\\dots & x_{N,p}\n",
    "\\end{pmatrix} \\implies\n",
    "\\mathbf{X}^\\top = \\begin{pmatrix}\n",
    "x_{1,1}&\\dots & x_{p,1} \\\\\n",
    "x_{1,2}&\\dots & x_{p,2} \\\\\n",
    "& \\vdots & \\\\\n",
    "x_{1,N}&\\dots & x_{p,N}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "### Matrix multiplication\n",
    "\n",
    "[matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) is an operation that takes a pair of matrices, and produces another matrix. If A is an n × m matrix and B is an m × p matrix, their matrix product AB is an n × p matrix. The number rows of the first matrix must match the columns of the second.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a + b + c &=5\\\\\n",
    "3a - 2b + c &= 3\\\\\n",
    "2a + b  - c &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The idea is to multiply the rows of the first matrix by the columns of the second.\n",
    "\n",
    "$$\n",
    "\\,\n",
    "\\begin{pmatrix}\n",
    "1&1&1\\\\\n",
    "3&-2&1\\\\\n",
    "2&1&-1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "a + b + c \\\\\n",
    "3a - 2b + c \\\\\n",
    "2a + b  - c\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### Adding vectors\n",
    "\n",
    "\n",
    "![image vector addition A](http://nikbearbrown.com/YouTube/MachineLearning/M03/Vector_addition_M03.png)   \n",
    "\n",
    "\n",
    "### Unit vector\n",
    "\n",
    "A unit vector in a normed vector space is a vector of length 1. A unit vector is often denoted by a lowercase letter with a \"hat\": i-hat (pronounced \"i-hat\"). The normalized vector or versor û of a non-zero vector u is the unit vector in the direction of u, i.e.,\n",
    "\n",
    "$$\\mathbf{\\hat{u}} = \\frac{\\mathbf{u}}{\\|\\mathbf{u}\\|}$$\n",
    "\n",
    "u-hat equals the vector u divided by its length where ||u|| is the norm (or length) of u. The term normalized vector is sometimes used as a synonym for unit vector.\n",
    "\n",
    "from [Unit vector - Wikipedia](https://en.wikipedia.org/wiki/Unit_vector)\n",
    "\n",
    "### Linear combinations \n",
    "\n",
    "We can represent any vector with a linear combination of other vectors. If $\\vec{V}$ are vectors $v_1,...,v_n$ and A are scalars $a_1,...,a_n$ then the linear combination of those vectors with those scalars as coefficients is\n",
    "\n",
    "$$ a_1 \\vec{v}_{1} \\quad + \\quad a_2 \\vec{v}_2  \\quad + \\quad  a_3 \\vec{v}_3  \\quad + \\quad  \\cdots  \\quad + \\quad  a_n \\vec{v}_n. $$  \n",
    "\n",
    "For example, consider the vectors $\\vec{v}_1 = (1,0,0),  \\quad \\vec{v}_2 = (0,1,0)  \\quad and  \\quad \\vec{v}_3 = (0,0,1)$. Then any vector in $\\mathbb{R}^3$ is a linear combination of $\\vec{v}_1,  \\quad \\vec{v}_2  \\quad and  \\quad \\vec{v}_3$.\n",
    "To see that this is so, take an arbitrary vector (a1,a2,a3) in $\\mathbb{R}^3,$, and write:\n",
    "\n",
    "$$ ( a_1 , a_2 , a_3) = ( a_1 ,0,0) + (0, a_2 ,0) + (0,0, a_3)   =  a_1 (1,0,0) + a_2 (0,1,0) + a_3 (0,0,1) =  a_1 \\vec{v}_1 +  a_2 \\vec{v}_2 +  a_3 \\vec{v}_3.   $$\n",
    "\n",
    "![image Linear combination Standard basis A](http://nikbearbrown.com/YouTube/MachineLearning/M03/3D_Vector_Linear_Combo.png)   \n",
    "\n",
    "### Linear Spans and Basis\n",
    "\n",
    "The span of S may be defined as the set of all finite linear combinations of elements of S,\n",
    "\n",
    "$$ \\operatorname{span}(S) =  \\left \\{ {0+\\sum_{i=1}^k \\lambda_i v_i \\Big| k \\in \\mathbb{N}, v_i  \\in S, \\lambda _i  \\in \\mathbf{K}} \\right \\} $$  \n",
    "\n",
    "A set of vectors in a vector space V is called a [basis](https://en.wikipedia.org/wiki/Basis_(linear_algebra)), or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set. In more general terms, a basis is a linearly independent spanning set.\n",
    "\n",
    "For example, the real vector space $\\mathbb{R}^3$ has {(5,0,0), (0,3,0), (0,0,9)} is a spanning set. This particular spanning set is also a basis. \n",
    "\n",
    "The set {(1,0,0), (0,1,0), (1,3,0)} is not a spanning set of $\\mathbb{R}^3$ as a vector like (1,3,1) cannot be created from this spanning set.\n",
    "\n",
    "### Linear subspaces\n",
    "\n",
    "A [linear subspace](https://en.wikipedia.org/wiki/Linear_subspace) (or vector subspace) is a vector space that is a subset of some other (higher-dimension) vector space. For example, $\\mathbb{R}^2$ is a subspace of $\\mathbb{R}^3$.\n",
    " \n",
    "Let V be a vector space over the field K, and let W be a subset of V. Then W is a subspace if and only if W satisfies the following three conditions:  \n",
    "\n",
    "*  The zero vector, 0, is in W.\n",
    "*  If u and v are elements of W, then the sum u + v is an element of W;\n",
    "*  If u is an element of W and c is a scalar from K, then the product cu is an element of W;\n",
    "\n",
    "\n",
    "### Metric Spaces\n",
    "\n",
    "A metric space is an ordered pair (M,d) where M is a set and d is a metric on M, i.e., a function * $d \\colon M \\times M \\rightarrow \\mathbb{R}$\n",
    "such that for any * $x, y, z \\in M$, the following holds:[1]\n",
    "\n",
    "* $d(x,y) \\ge 0$     (non-negative),  \n",
    "* $d(x,y) = 0\\, \\iff x = y\\,$     (identity of indiscernibles),  \n",
    "* $d(x,y) = d(y,x)\\,$     (symmetry) and  \n",
    "* $d(x,z) \\le d(x,y) + d(y,z) $    (triangle inequality) .  \n",
    "\n",
    "Examples of metric spaces include  [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry), [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance), the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) and many others.\n",
    "\n",
    "### Vector dot product\n",
    "\n",
    "*Algebraic definition*\n",
    "\n",
    "The dot product of two vectors A = [A1, A2, ..., An] and B = [B1, B2, ..., Bn] is defined as:\n",
    "\n",
    "$$ \\mathbf{A}\\cdot \\mathbf{B} = \\sum_{i=1}^n A_iB_i = A_1B_1 + A_2B_2 + \\cdots + A_nB_n $$ \n",
    "\n",
    "\n",
    "*Geometric definition*  \n",
    "\n",
    "he magnitude of a vector A is denoted by  \\left\\| \\mathbf{A} \\right\\| . The dot product of two Euclidean vectors A and B is:\n",
    "\n",
    "\n",
    "$$\\mathbf A \\cdot \\mathbf B = \\left\\| \\mathbf A \\right\\| \\, \\left\\| \\mathbf B \\right\\| \\cos \\theta ,$$\n",
    "\n",
    "where $\\theta$ is the angle between A and B.\n",
    "\n",
    "![image dot product  A](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/440px-Dot_Product.svg.png)  \n",
    "\n",
    "\n",
    "### Orthogonal and orthonormal\n",
    "\n",
    "Two vectors are orthogonal if they are perpendicular, i.e., they form a right angle. Two vectors orthonormal if they are orthogonal and unit vectors. Orthogonal vectors in n-space if their dot product equals zero. That is, if A and B are orthogonal, then the angle between them is 90° and $\\mathbf A \\cdot \\mathbf B = 0 .$\n",
    "\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "\n",
    "In linear algebra, an eigenvector or characteristic vector of a square matrix is a vector that does not change its direction under the associated linear transformation. In other words—if v is a vector that is not zero, then it is an eigenvector of a square matrix A if Av is a scalar multiple of v. This condition could be written as the equation\n",
    "\n",
    "$$A\\vec{v} = \\lambda \\vec{v}$$\n",
    " \n",
    "\n",
    "where $\\lambda$ is a number (also called a scalar) known as the eigenvalue or characteristic value associated with the eigenvector $\\vec{v}$.   \n",
    "\n",
    "![image Mona Lisa eigenvector](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Mona_Lisa_eigenvector_grid.png/480px-Mona_Lisa_eigenvector_grid.png)   \n",
    "\n",
    "In this shear mapping the red arrow changes direction but the blue arrow does not. The blue arrow is an eigenvector of this shear mapping because it doesn't change direction, and since its length is unchanged, its eigenvalue is 1.  \n",
    "\n",
    "For example, Consider n-dimensional vectors that are formed as a list of n real numbers, such as the three dimensional vectors,\n",
    "\n",
    "$$ \n",
    "\\mathbf{u} = \\begin{Bmatrix}1\\\\3\\\\4\\end{Bmatrix}\\quad\\mbox{and}\\quad \\mathbf{v} = \\begin{Bmatrix}-20\\\\-60\\\\-80\\end{Bmatrix}. $$ \n",
    "\n",
    "These vectors are said to be scalar multiples of each other, also parallel or collinear, if there is a scalar λ, such that\n",
    "\\mathbf{u}=\\lambda\\mathbf{v}.\n",
    "\n",
    "In this case $\\lambda\\$ = −1/20.  \n",
    "\n",
    "Now consider the linear transformation of n-dimensional vectors defined by an n×n matrix A, that is,\n",
    "\n",
    "$$\n",
    " A\\mathbf{v}=\\mathbf{w},\n",
    "or\n",
    "\\begin{bmatrix} A_{1,1} & A_{1,2} & \\ldots & A_{1,n} \\\\\n",
    "A_{2,1} & A_{2,2} & \\ldots & A_{2,n} \\\\\n",
    "\\vdots &  \\vdots &  \\ddots &  \\vdots \\\\\n",
    "A_{n,1} & A_{n,2} & \\ldots & A_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{Bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{Bmatrix} = \\begin{Bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{Bmatrix}\n",
    "where, for each index i,\n",
    " w_i = A_{i,1} v_1 + A_{i,2} v_2 + \\cdots + A_{i,n} v_n = \\sum_{j = 1}^{n} A_{i,j} v_j.\n",
    " \n",
    " $$\n",
    "If it occurs that w and v are scalar multiples, that is if\n",
    "$A\\mathbf{v}=\\lambda\\mathbf{v}$,\n",
    "then v is an eigenvector of the linear transformation A and the scale factor $\\lambda\\$ is the eigenvalue corresponding to that eigenvector.\n",
    "\n",
    "\n",
    "![image eigenvector](https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/500px-Eigenvalue_equation.svg.png)   \n",
    "\n",
    "Matrix A acts by stretching the vector x, not changing its direction, so x is an eigenvector of A.\n",
    "\n",
    "from [Eigenvalues and eigenvectors - Wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)\n",
    "\n",
    "\n",
    "# PCA: Principal Component Analyses\n",
    "\n",
    "[Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "In Principal component analysis the feature selection is finding a set of linearly uncorrelated vectors (basis selection) of maximal variance. That is, the transformation is defined in such a way that the first principal component has the largest possible variance and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. \n",
    "\n",
    "![image  Principal Component Analyses](https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/GaussianScatterPCA.png/440px-GaussianScatterPCA.png)  \n",
    "\n",
    "*First component*  \n",
    "\n",
    "The first loading vector $\\vec{w}$ thus has to satisfy  \n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(1)}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\,\\left\\{ \\sum_i \\left(t_1\\right)^2_{(i)} \\right\\}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\,\\left\\{ \\sum_i \\left(\\mathbf{x}_{(i)} \\cdot \\mathbf{w} \\right)^2 \\right\\} $$ \n",
    " \n",
    "Equivalently, writing this in matrix form gives\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(1)}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\, \\{ \\Vert \\mathbf{Xw} \\Vert^2 \\}\n",
    " = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\, \\left\\{ \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X w} \\right\\} $$  \n",
    " \n",
    " \n",
    "Since $\\vec{w}$ has been defined to be a unit vector, it equivalently also satisfies  \n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(1)} = {\\operatorname{\\arg\\,max}}\\, \\left\\{ \\frac{\\mathbf{w}^T\\mathbf{X}^T \\mathbf{X w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\}\n",
    "$$\n",
    "\n",
    "The first component is an eigenvector that maximizes the sum of squares\n",
    "\n",
    "$$(\\mathbf{Yv}_1)^\\top \\mathbf{Yv}_1$$\n",
    "\n",
    "$\\mathbf{v}_1$ is referred to as the _first principal component_ (PC). Also referred as  _first eigenvector_, $\\mathbf{Yv}_1$ are the projections or coordinates or eigenvalues\n",
    "\n",
    "*Further components*  \n",
    "\n",
    "The kth component can be found by subtracting the first k − 1 principal components from X:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{X}}_{k} = \\mathbf{X} - \\sum_{s = 1}^{k - 1} \\mathbf{X} \\mathbf{w}_{(s)} \\mathbf{w}_{(s)}^{\\rm T} $$  \n",
    "\n",
    "and then finding the loading vector which extracts the maximum variance from this new data matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{(k)} = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{arg\\,max}} \\left\\{ \\Vert \\mathbf{\\hat{X}}_{k} \\mathbf{w} \\Vert^2 \\right\\} = {\\operatorname{\\arg\\,max}}\\, \\left\\{ \\tfrac{\\mathbf{w}^T\\mathbf{\\hat{X}}_{k}^T \\mathbf{\\hat{X}}_{k} \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\} $$  \n",
    "\n",
    "It turns out that this gives the remaining eigenvectors of $X^TX$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues.\n",
    "\n",
    "The kth  is the vector that\n",
    "\n",
    "$$ \\mathbf{v}_{k}^\\top \\mathbf{v}_{k}=1$$\n",
    "\n",
    "$$ \\mathbf{v}_{k}^\\top \\mathbf{v}_{k-1}=0$$\n",
    "\n",
    "and maximizes  $$(\\mathbf{rv}_{k})^\\top \\mathbf{rv}_{k}$$\n",
    "\n",
    "\n",
    "### Properties of Principal Components\n",
    "\n",
    "* The principal components are eigenvectors\n",
    "* Maximizes variance in order of the components\n",
    "* They are orthogonal (and form a basis)\n",
    "* If use all of the principal components  we can get lossless reconstruction \n",
    "* N diminsions to M diminsions (each diminsion has an eigenvalue \n",
    "the order (highest eigenvalues have highest  variance)\n",
    "i.e. can \"throw away\" the lowest eigenvalues.\n",
    "if eigenvalue has 0 value can throw it away without cost.\n",
    "* We can readjust to origin 0\n",
    "* There are very fast algorithms to compute PCA\n",
    "\n",
    "*Nota bene*  \n",
    "Note that there may be oroblems of interpretation of the components.\n",
    "Note that a low variance dimension may be important\n",
    "\n",
    "\n",
    "## LDA: Linear Discriminant Analysis\n",
    "\n",
    "\n",
    "LDA: Linear Discriminant Analysis\n",
    "\n",
    "An alternative method to calculate eigenvectors from covariance matrix\n",
    "[Linear discriminant analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n",
    "\n",
    "\n",
    "Singular Value Decomposition\n",
    "\n",
    "In linear algebra, the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) is a factorization of a real or complex matrix. A  a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices.\n",
    "\n",
    "If $\\mathbf{M}$ is a m × n matrix whose entries come from $\\mathbb{R}$, then the SVD  a factorization, called a singular value decomposition of  $\\mathbf{M}$ , of the form $\\mathbf{M=UDV}^\\top$ gives the factors in columns of $\\mathbf{V}$.\n",
    "\n",
    "Note that,  \n",
    "$\\mathbf{D}$ is a m × n diagonal matrix with non-negative real numbers on the diagonal, and  \n",
    "$\\mathbf{U}$  is an m × m, and $\\mathbf{V}$ is an n × n, unitary matrix over $\\mathbb{R}$.\n",
    "\n",
    "The diagonal entries, $\\sigma_{i}$, of $D$ are known as the singular values of $\\mathbf{M}$. The singular values are usually listed in descending order.\n",
    "\n",
    "Note that [Eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) is a form of matrix factorization.   \n",
    "\n",
    "$A=VDV^{-1}$, where D is a diagonal matrix formed from the eigenvalues of A, and the columns of V are the corresponding eigenvectors of A.\n",
    "\n",
    "### Differences between Principal Component Analysis and Singular-value decomposition\n",
    "\n",
    "The singular value decomposition and the eigendecomposition are closely related.  PCA viewpoint requires that one compute the eigenvalues and eigenvectors of the covariance matrix, which is the product $\\mathbf{X}\\mathbf{X^T}$, where $\\mathbf{X}$ is the data matrix and SVD constructs the covariance matrix from this decomposition\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{X^T}= \\mathbf{(UDV^T)}\\mathbf{(UDV^T)}^T$$\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{X^T}= \\mathbf{(UDV^T)}\\mathbf{(VDU^T)}$$  \n",
    "\n",
    "and since $\\mathbf{V}$ is an orthogonal matrix $\\mathbf{V^TV}=\\mathbf{I}$),\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{X^T}= \\mathbf{U}\\mathbf{D}^2\\mathbf{U^T}$$\n",
    "\n",
    "That is,  \n",
    "- The left-singular vectors of $\\mathbf{M}$ are eigenvectors of $\\mathbf{M}\\mathbf{M^*}$ .  \n",
    "- The right-singular vectors of  $\\mathbf{M}$ are eigenvectors of $\\mathbf{M^*}\\mathbf{M}$.  \n",
    "- The non-zero singular values of $\\mathbf{M}$  (found on the diagonal entries of $\\mathbf{D}$) are the square roots of the non-zero eigenvalues of both $\\mathbf{M^*}\\mathbf{M}$ and $\\mathbf{M}\\mathbf{M^*}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization, in mathematics and statistics and particularly in the fields of machine learning and inverse problems, is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.\n",
    "\n",
    "- Regularization is a method for \"constraining\" or \"regularizing\" the **size of the coefficients**, thus \"shrinking\" them towards zero.\n",
    "- It reduces model variance and thus **minimizes overfitting**.\n",
    "- If the model is too complex, it tends to reduce variance more than it increases bias, resulting in a model that is **more likely to generalize**.\n",
    "\n",
    "\n",
    "### Regularization in linear regression\n",
    "\n",
    "For a normal linear regression model, we estimate the coefficients using the least squares criterion, which **minimizes the residual sum of squares (RSS):**\n",
    "\n",
    "For a regularized linear regression model, we **minimize the sum of RSS and a \"penalty term\"** that penalizes coefficient size.\n",
    "\n",
    "**Ridge regression** (or \"L2 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Lasso regression** (or \"L1 regularization\") minimizes: $$\\text{RSS} + \\alpha \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "- $p$ is the **number of features**\n",
    "- $\\beta_j$ is a **model coefficient**\n",
    "- $\\alpha$ is a **tuning parameter:**\n",
    "    - A tiny $\\alpha$ imposes no penalty on the coefficient size, and is equivalent to a normal linear regression model.\n",
    "    - Increasing the $\\alpha$ penalizes the coefficients and thus shrinks them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. \n",
    "\n",
    "**k-fold cross-validation**  \n",
    "\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[7] but in general k remains an unfixed parameter.\n",
    "\n",
    "For example, setting k = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and test on d1, followed by training on d1 and testing on d0.\n",
    "\n",
    "When k = n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.\n",
    "\n",
    "In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time Series  \n",
    "\n",
    "A [time series](https://en.wikipedia.org/wiki/Time_series) is a sequence of data points, typically consisting of successive measurements made over a time interval. This is a very common type of data as we frequently measure how something varies over time.  usually a time series is an ordered sequence of values of a variable at equally spaced time intervals; but methods exist to deal with irregular sampling.  \n",
    "\n",
    "Examples of time series include:\n",
    "\n",
    "* Stock Market\n",
    "* The change of weather \n",
    "* An electrocardiogram (EKG or ECG), that is, the electrical activity of your heart.\n",
    "* The popularity of a celebirty or politician\n",
    "* Economic Forecasting  \n",
    "\n",
    "![Gold Spot Price](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Gold_Spot_Price_per_Gram_from_Jan_1971_to_Jan_2012.png)   \n",
    "\n",
    "- from https://commons.wikimedia.org/wiki/File:Gold_Spot_Price_per_Gram_from_Jan_1971_to_Jan_2012.svg\n",
    "\n",
    "\n",
    "Univariate (bivariate, multivariate) time series: collection of observations of one (two, several) state variables, that are made in sequential momentsin time.\n",
    "\n",
    "The [sampling](https://en.wikipedia.org/wiki/Sampling_(signal_processing)) frequency (or sample rate) is the number of samples per unit of time. (e.g once per minute).\n",
    "\n",
    "\n",
    "![Signal Sampling](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Signal_Sampling.png)   \n",
    "- from https://commons.wikimedia.org/wiki/File:Signal_Sampling.png  \n",
    "\n",
    "### Nyquist criterion\n",
    "\n",
    "Nyquist criterion, or [Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist_frequency), named after electronic engineer [Harry Nyquist](https://en.wikipedia.org/wiki/Harry_Nyquist), is half of the sampling rate of a discrete signal processing system. In order to correctly determine the frequency spectrum of a signal, the signal must be measured at least twice per period.\n",
    "\n",
    "### Reconstruction \n",
    "\n",
    "Reconstruction is the opposite of sampling, the process by which a sampled signal is converted into a continuous signal.  \n",
    "\n",
    "![Under-Sampling](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Under-Sampling.png)    \n",
    "Under-Sampling  \n",
    "\n",
    "### Components of Time Series\n",
    "\n",
    "The pattern or behavior of the data in a time series can be made up of several components. Theoretically, any time series can be decomposed into:\n",
    "\n",
    "*Trend*   \n",
    "\n",
    "The trend component accounts for the gradual shifting of the time series to relatively higher or lower values over a long period of time.\n",
    "\n",
    "*Cyclical*\n",
    "\n",
    "A regular pattern of sequences of values that go above and below the trend line is a cyclical component.\n",
    "\n",
    "\n",
    "*Seasonal*\n",
    "\n",
    "The seasonal component accounts for regular patterns of variability within certain time periods, such as a year. While seasons could be modeled using cyclical components. Often within-year, within-month, within-week or within-day cycles are treated as “seasonal” behavior.  \n",
    "\n",
    "\n",
    "*Irregular*  \n",
    "\n",
    "The irregular components represent erratic, unsystematic, ‘residual’ fluctuations. That is, noise.   \n",
    "\n",
    "\n",
    "### Stationary versus Non-stationary time series\n",
    "\n",
    "Non-stationary time series or have means, variances and covariances that change over time. Whereas while stationary time series have up and down flucutions they have means, variances and covariances.  That is, a [stationary process](https://en.wikipedia.org/wiki/Stationary_process) is a stochastic process whose joint probability distribution does not change when shifted in time. \n",
    "\n",
    "For example, we expect financial time series data, to be non-stationary since we would expect a deterministic upward trend in price.\n",
    "\n",
    "A grown adults weight would more likely be stationary time series unless he or she fundenemtally changes their \"state\".  \n",
    "\n",
    "![Stationary comparison](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Stationarycomparison.png)     \n",
    "*Two simulated time series processes, one stationary the other non-stationary.* \n",
    "- from https://commons.wikimedia.org/wiki/File:Stationarycomparison.png   \n",
    "\n",
    "### Smoothing Methods of Time Series\n",
    "\n",
    "Smoothing a time series: to eliminate some of short-term fluctuations. Sometimes smoothing is done to remove cyclical or  seasonal fluctuations. (i.e. \"deseasonalize\"\" a time series). Usually smoothing refers to methods for reducing of canceling the effect due to random variation. The simplest smoothing method is just the \"simple\" average of all past data.  Of course, the \"simple\" average or mean of all past observations is only a useful estimate for forecasting when there are no trends. \n",
    "\n",
    "### Time Series Decomposition\n",
    "\n",
    "Time Series Decomposition is a procedure to identify the component factors of a time series. That is, to create a model that expresses the time series variable Y in terms of the components T (trend), C (cycle), S (seasonal) and I (iregular).\n",
    "\n",
    "## Time Series Analysis\n",
    "\n",
    "Time series analysis generates a model that accounts an internal structure (such as autocorrelation, trend or seasonal variation) of a set of data points taken over time. This is very frequently used for forecasting. That is, predicting and future event in time based on a sequential historical sample.  \n",
    "\n",
    "\n",
    "### Autocorrelation\n",
    "\n",
    "[Autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation), also known as serial correlation or cross-autocorrelation, is the cross-correlation of a signal with itself at different points in time. It is the correlation between values of the process at different times. Informally, it is the similarity between observations as a function of the time lag between them. It is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise.  \n",
    "\n",
    "Let X be some repeatable process, and i be some point in time after the start of that process. (i may be an integer for a discrete-time process or a real number for a continuous-time process.) Then $X_i$ is the value (or realization) produced by a given run of the process at time i. Suppose that the process is further known to have defined values for mean μi and variance σi2 for all times i. Then the definition of the autocorrelation between times s and t is\n",
    "\n",
    "$$\n",
    "R(s,t) = \\frac{\\operatorname{E}[(X_t - \\mu_t)(X_s - \\mu_s)]}{\\sigma_t\\sigma_s}\\, ,\n",
    "$$\n",
    "\n",
    "where \"E\" is the expected value operator. Note that this expression is not well-defined for all time series or processes, because the variance may be zero (for a constant process) or infinite (for processes with distribution lacking well-behaved moments, such as certain types of power law).\n",
    "\n",
    "### Correlogram\n",
    "\n",
    "In the analysis of data, a [correlogram](https://en.wikipedia.org/wiki/Correlogram) is an image of correlation statistics. For example, in time series analysis, a correlogram, also known as an autocorrelation plot, is a plot of the sample autocorrelations $r_{h}, r_{h}$, versus $h$ (the time lags).  \n",
    "\n",
    "If cross-correlation is used, the result is called a cross-correlogram. The correlogram is a commonly used tool for checking randomness in a data set. This randomness is ascertained by computing autocorrelations for data values at varying time lags. If random, such autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero.\n",
    "\n",
    "![Correlogram](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Correlogram.png)   \n",
    "\n",
    "[By M. W. Toews - Own work, CC BY 4.0](https://commons.wikimedia.org/w/index.php?curid=2379384)\n",
    "\n",
    "### Naive & Simple Averaging\n",
    "\n",
    "Naive or \"simple\" averaging is just the mean (or median) of all past data.  Of course, the \"simple\" average or mean of all past observations is only a useful estimate for forecasting when there are no trends. \n",
    "\n",
    "For example, if we have prices are $p_M, p_{M-1},\\dots,p_{M-(n-1)}$ then the formula is\n",
    "\n",
    "$$\n",
    "\\textit{SMA} = { p_M + p_{M-1} + \\cdots + p_{M-(n-1)} \\over n }\n",
    "$$\n",
    "\n",
    "### Moving Averages\n",
    "\n",
    "A moving average (rolling average or running average) is an average that is updated for a window or history of n events. This is sometimes called the arithmetic moving average the most recent n data values. For an an equally weighted average of the sequence of n values $x_1. \\ldots, x_n$ up to the current time:\n",
    "$$\n",
    "\\textit{MA}_n = {{x_1 + \\cdots + x_n} \\over n}\\,.\n",
    "$$\n",
    "\n",
    "### Weighted Moving Averages\n",
    "\n",
    "A weighted moving average places more weight on recent observations.  Sum of the weights needs to equal 1. For example. in technical analysis of financial data, a weighted moving average (WMA) often uses weights that decrease in arithmetical progression. In an n-day WMA the latest day has weight n, the second latest n − 1, etc., down to one.\n",
    "\n",
    "$$\n",
    "\\text{WMA}_{M} = { n p_{M} + (n-1) p_{M-1} + \\cdots + 2 p_{(M-n+2)} + p_{(M-n+1)} \\over n + (n-1) + \\cdots + 2 + 1}\n",
    "$$\n",
    "\n",
    "That is, the denominator is a triangle number equal to $\\frac{n(n+1)}{2}$.\n",
    "\n",
    "There are many other methods for weighting the importance of more recent observations.   \n",
    "\n",
    "![Weighted moving average weights N=15](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Weighted_moving_average_weights_N=15.png)   \n",
    "\"Weighted moving average weights N=15\". Licensed under CC BY-SA 3.0 via Commons   \n",
    "\n",
    "\n",
    "### Exponential moving average\n",
    "\n",
    "Exponential moving average (also called Single Exponential Smoothing) continually revises a forecast in light of more recent experiences.  Averaging (smoothing) past values of a series in a decreasing (exponential) manner.  \n",
    "\n",
    "The EMA for a series Y may be calculated recursively:\n",
    "\n",
    "$$\n",
    "S_1   Y_1 \\quad for  \\quad t > 1, S_{t} = \\alpha \\cdot Y_{t} + (1-\\alpha) \\cdot S_{t-1}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "\n",
    "* The coefficient $\\alpha$ represents the degree of weighting decrease, a constant smoothing factor between 0 and 1. A higher  $\\alpha$  discounts older observations faster.  \n",
    "* $Y_t$ is the value at a time period t.\n",
    "* $S_t$ is the value of the EMA at any time period t.\n",
    "\n",
    "Since the base values of the recursive intial call is undefined. The priming of values such as $S_1$ may be initialized in a number of different ways, most commonly by setting $S_1$ to $Y_1$, or by setting  $S_1$to an average of the first few observations.   \n",
    "\n",
    "![Exponential moving average weights N=15](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Exponential_moving_average_weights_N=15.png)    \n",
    "\"Exponential moving average weights N=15\". Licensed under CC BY-SA 3.0 via Commons \n",
    "\n",
    "### Double exponential smoothing  \n",
    "\n",
    "Holt-Winters method for exponential smoothing extended Holt’s method to capture seasonality. This is sometimes called \"Double exponential smoothing.\" The Holt-Winters seasonal method comprises the forecast equation and two smoothing equations. e use {$s_t$} to represent the smoothed value for time t, and {$b_t$} is our best estimate of the trend at time t. The output of the algorithm is now written as Ft+m, an estimate of the value of x at time t+m, m>0 based on the raw data up to time t. Double exponential smoothing is given by the formulas\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s_1& = x_1\\\\\n",
    "b_1& = x_1 - x_0\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And for t > 1 by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s_{t}& = \\alpha x_{t} + (1-\\alpha)(s_{t-1} + b_{t-1})\\\\\n",
    "b_{t}& = \\beta (s_t - s_{t-1}) + (1-\\beta)b_{t-1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where α is the data smoothing factor, 0 < $\\alpha$ < 1, and β is the trend smoothing factor, 0 < $\\beta$ < 1. \n",
    "\n",
    "### Triple exponential smoothing\n",
    "\n",
    "Triple exponential smoothing account seasonal changes as well as trends using three smoothing equations — one for the level, one for trend, and one for the seasonal component denoted, with smoothing parameters $\\alpha, \\beta,$ and $\\gamma$. We use a variable $m$ to denote the period of the seasonality, i.e., the number of seasons in a year. For example, for quarterly data m=4, and for monthly data m=12.\n",
    "\n",
    "There are two variations to this method that differ in the nature of the seasonal component. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series. \n",
    "\n",
    "\n",
    "## Autoregressive integrated moving average (ARIMA)\n",
    "\n",
    "An autoregressive integrated moving average (ARIMA or ARMA) model combines an autoregressive component with a moving average component in to a single model.   \n",
    "\n",
    "An [autoregressive integrated moving average (ARIMA or ARMA)](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model is a generalization of an autoregressive moving average (ARMA) model. These models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting). They are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the \"integrated\" part of the model) can be applied to reduce the non-stationarity.\n",
    "\n",
    "Non-seasonal ARIMA models are generally denoted $ARIMA(p, d, q)$ where parameters $p, d, and q$ are non-negative integers, $p$ is the order of the Autoregressive model, $d$ is the degree of differencing, and $q$ is the order of the Moving-average model.  The he number of differences $d$ is determined using repeated statistical tests. The values of $p$ and $q$ are then chosen by minimizing the AICc after differencing the data $d$ times. \n",
    "\n",
    "The ARIMA model uses an iterative three-stage modeling approach:\n",
    "\n",
    "Model identification and model selection: making sure that the variables are stationary, identifying seasonality in the dependent series (seasonally differencing it if necessary), and using plots of the autocorrelation and partial autocorrelation functions of the dependent time series to decide which (if any) autoregressive or moving average component should be used in the model.   \n",
    "\n",
    "Parameter estimation using computation algorithms to arrive at coefficients that best fit the selected ARIMA model. The most common methods use maximum likelihood estimation or non-linear least-squares estimation.   \n",
    "\n",
    "Model checking by testing whether the estimated model conforms to the specifications of a stationary univariate process. In particular, the residuals should be independent of each other and constant in mean and variance over time. (Plotting the mean and variance of residuals over time and performing a Ljung-Box test or plotting autocorrelation and partial autocorrelation of the residuals are helpful to identify misspecification.) If the estimation is inadequate, we have to return to step one and attempt to build a better model.   \n",
    "\n",
    "## Linear Time-Series Model\n",
    "\n",
    "Using regression we can model and forecast the trend in time series data by including the time as a predictor variable. Time series processes are often described by multiple linear regression (MLR) models of the form:\n",
    "\n",
    "$$y_t = X_t \\beta + e_t,$$\n",
    "\n",
    "Note that the above equation differs from our usual regression in that the predictor variables are a function of time, $t$. As oppossed to the more familair form below:  \n",
    "\n",
    "$$ Y = \\beta X + e $$\n",
    "\n",
    "where  $y_t$ is an observed response and  $X_t$ includes columns for contemporaneous values of observable predictors. The partial regression coefficients in  $\\beta$ represent the marginal contributions of individual predictors to the variation in  $y_t$ when all of the other predictors are held fixed. A common feature of time series data is a trend. Regressing non-stationary time series can lead to spurious regressions.  \n",
    "\n",
    "The error term $\\varepsilon_i$ still has the following assumptions:\n",
    "\n",
    "* have mean zero; otherwise the forecasts will be systematically biased.\n",
    "* statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data).\n",
    "* homoscedasticity (constant variance) of the errors.\n",
    "* normality of the error distribution.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text Mining   \n",
    "\n",
    "[Text mining](https://en.wikipedia.org/wiki/Text_mining), also called text analytics, is the process of deriving high-quality information from text. Given the vast amount of text on the Internet, text mining is one of the most important research areas in machine learning. Text mining includes:\n",
    "\n",
    "* [Information retrieval](https://en.wikipedia.org/wiki/Information_retrieval) - the process of obtaining structured data from free text.\n",
    "* [Entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) - identification of nouns (e.g. known people, places and things) in text. \n",
    "* Fact extraction - identification of associations among entities and other information in text.\n",
    "* [Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is the assocaition of opinion and a topic within text. \n",
    "* [Topic modeling](https://en.wikipedia.org/wiki/Topic_model) - identification of \"topics\" in text.\n",
    "* Automated Tagging - a process to associate free-form keywords (not necessarily in the text) to index and search text.\n",
    "\n",
    "\n",
    "##  Text corpora and dictionaries  \n",
    "\n",
    "In linguistics, a corpus (plural corpora) or text corpus is a large and structured set of texts. They help annotate words.  \n",
    "\n",
    "* [corpus.byu.edu](http://corpus.byu.edu/) includes:\n",
    "     + Hansard Corpus (British Parliament), 1.6 billion words\n",
    "     + Global Web-Based English (GloWbE), 1.9 billion words  \n",
    "     + Corpus of Contemporary American English (COCA), 450 million words  \n",
    "     + TIME Magazine Corpus, 100 million words  \n",
    "     + Corpus of American Soap Operas, 100 million words  \n",
    "     + British National Corpus (BYU-BNC), 100 million words  \n",
    "     + Strathy Corpus (Canada), 100 million words  \n",
    "* The [British National Corpus](http://www.natcorp.ox.ac.uk/), extracts from 4124 modern British English texts of all kinds, both spoken and written; over 100 million words.\n",
    "* The [Brown University Corpus](http://www.hit.uib.no/icame/brown/bcm.html): Approximately 1,000,000 words of American written English dating from 1960. The genre categories are parallel to those of the LOB corpus.\n",
    "* The [LOB Corpus](http://clu.uni.no/icame/manuals/) (The Lancaster-Oslo/Bergen Corpus) is ) is a million-word collection of British English texts which was compiled in the 1970s.\n",
    "* [The Kolhapur Corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/kolhapur.html): Approximately 1,000,000 words of Indian written English dating from 1978.  \n",
    "* The (Cambridge International Corpus)[http://uk.cambridge.org/elt/corpus] is a multi-billion word corpus of English language (containing both text corpus and spoken corpus)  \n",
    "* [The Longman-Lancaster Corpus](http://www.pearsonlongman.com/dictionaries/corpus/lancaster.html): Approximately 14.5 million words of written English from various geographical locations in the English-speaking world and of various dates and text types.* [WordNet](http://wordnet.princeton.edu/) is a lexical database of English nuns, verbs, adjectives and adverbs which are grouped into sets of cognitive synonyms (synsets). The WordNet synsets are further characterized by hyperonymy, hyponymy or ISA relationships. We downloaded the WordNet database files and parsed them. Permission to use, copy, modify and distribute WordNet for any purpose and without fee or royalty is hereby granted, WordNet provided by WordNet as long as proper attribution is given to WordNet and any derivative products don’t use the WordNet trademark.  \n",
    "* [PubMed/Medline](http://www.ncbi.nlm.nih.gov/pubmed) comprises more than 25 million citations for biomedical literature. PubMed XML Data Retrieved from [http://www.nlm.nih.gov/databases/journal.html](http://www.nlm.nih.gov/databases/journal.html). You need to regiester with the National Library of Medicine to download the XML files.    \n",
    "* [arXiv](http://arxiv.org/) is an  archive with over 100000 articles in physics, 10000 in mathematics, and 1000 in computer science. arXiv Bulk Data Access Retrieved from [http://arxiv.org/help/bulk_data](http://arxiv.org/help/bulk_data)  \n",
    "* AG's news corpus is AG's corpus of news articles. Retrieved from [http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)  \n",
    "* Last.fm music tags can be retrieved from [http://www.last.fm/charts/toptags](http://www.last.fm/charts/toptags )   \n",
    "* Spambase can be retrieved from [http://archive.ics.uci.edu/ml/datasets/Spambase](http://archive.ics.uci.edu/ml/datasets/Spambase) \n",
    "* Wikipedia. The  Wikipedia Data Dump can be retrieved from [http://en.wikipedia.org/wiki/Wikipedia:Database_download](http://en.wikipedia.org/wiki/Wikipedia:Database_download)  \n",
    "\n",
    "###  Unstructured text data\n",
    "\n",
    "*  [Common Crawl](http://commoncrawl.org/) is a openly accessible web crawl data that is freely available. [164] As of April 2013 the crawl has 6 billion pages and associated metadata. The crawl data is stored on Amazon’s Public Data Sets, allowing it to be directly accessed for map-reduce processing in EC2. Common Crawl Retrieved from [http://commoncrawl.org/](http://commoncrawl.org/)  \n",
    "\n",
    "* Twitter data.\n",
    "    + Twitter Search API Retrieved from [https://dev.twitter.com/docs/api/1/get/search](https://dev.twitter.com/docs/api/1/get/search)\n",
    "    + Twitter Streaming APIs.  Retrieved from [https://dev.twitter.com/docs/streaming-apis](https://dev.twitter.com/docs/streaming-apis)\n",
    "    + Twitter“Fire hose” real-time stream. See [https://dev.twitter.com/streaming/firehose](https://dev.twitter.com/streaming/firehose)  \n",
    "    \n",
    "*  Instagram      \n",
    "    + API - Instagram [https://instagram.com/developer/](https://instagram.com/developer/)      \n",
    "\n",
    "###  Writing a web crawler  \n",
    "\n",
    "A Web crawler (also known as a Web spider or Web robot or  bot) is a pscript which browses the World Wide Web and extracts web pages and links.\n",
    "```\n",
    "urls=<list of urls>\n",
    "while (urls)\n",
    "{\n",
    "  * request url\n",
    "  * request document\n",
    "  * store text for later processing\n",
    "  * parse document for links\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "*Useful web crawling libraries in R*      \n",
    "\n",
    "*  library(regex) - Regular Expressions as used in R  \n",
    "*  library(httr) - Tools for Working with URLs and HTTP  \n",
    "*  library(XML) - XML Parser and Generator   \n",
    "*  library(RCurl) - RCurl: General Network (HTTP/FTP/...) Client Interface for R  \n",
    "*  library(jsonlite) - JSON Parser and Generator for R  \n",
    "*  library(stringr) - Simple, Consistent Wrappers for Common String Operations  \n",
    "\n",
    "## N-grams\n",
    "\n",
    "An [n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of n items from a given sequence of text or speech. An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". Larger sizes are sometimes referred to by the value of n, e.g., \"four-gram\", \"five-gram\", and so on.\n",
    "\n",
    "for example, bugaboo, nerd, student, data and new are one-grams. Data science, New York are two-grams. Note that New York means something different the the one grams new, and York seperately.\n",
    "\n",
    "\n",
    "Note that if we take a sentence, say\n",
    "\n",
    "\"You have brains in your head. You have feet in your shoes. You can steer yourself in any direction you choose. You're on your own, and you know what you know. And you are the guy who'll decide where to go.\"  \n",
    "\n",
    "- Dr. Seuss  \n",
    "\n",
    "Most of the two-grams aren't meaningful, that is, \"You have,\" \"have brains,\" \"brains in\" don't make sense out of context.\n",
    "\n",
    "\n",
    "## Tagging and Hashtags  \n",
    "\n",
    "Tagging is a process in which end users use free-form keywords to manually index content in an organic and distributed manner. The popularity of tagging has led some to claim that it is the primary classification scheme of the Internet. A tag can be thought of as an informative keyword. A user is very unlikely to tag an article with a word like “this” because it conveys very little information. Rather, they’ll often tag with a subject or sentiment.  \n",
    "\n",
    "Problems with tagging are well-known.  Users often present idiosyncrasies, inaccuracies, inconsistencies, and other irregularities when tagging. Specifically, four areas are critical to tagging. The first three areas are straightforward enough:\n",
    "\n",
    "1. tag misspelling;  \n",
    "2. tag heterogeneity, (that is, different tags denoting the same content, such as “Ziagen” and “abacavir sulfate,” which both refer to the same drug);  \n",
    "3. tag polysemy (i.e. identical tags that denote different meanings, such as, Apple may refer to fruit or a company. and;  \n",
    "4. semantic annotation of tags (i.e. abacavir sulfate is a drug).  \n",
    "\n",
    "\n",
    "An important area of research in text mining is call  “semantic enrichment” is a particularly difficult problem. Lexical resources are often used to annotate terms. For example, lexical databases such as WordNet is often used as a source of tag annotation. \n",
    "\n",
    "\n",
    "*##hashtags*\n",
    "\n",
    "The social tagging in is done by placing a hash mark in front of a word or phrase, such as ##BCSM, ##Lyphoma, ##BrainTumorThursday, ##BreastCancer, ##Infertility, ##Diabetes, ##lymphoedema, ##RareDiseaseDay, ##RareDisease, ##ADHD, ##Anorexia, ##MultipleSclerosis. On social media sites (such as Twitter) a word or phrase preceded by a hash or pound sign (##) and used to identify messages on a specific topic. Hastags are essentially tags that in within text and annotated.  As such they are easy to extract from text. \n",
    "\n",
    "*Stop words*\n",
    "\n",
    "In text mining, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out becuase they'll interfer with text analysis. Stop words usually refer to the most common words in a language, such as *the*, *is*, *at*, *which*, and on.   \n",
    "\n",
    "## Regular expressions\n",
    "\n",
    "Regular expressions (or RE or regex or regexp or rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching.\n",
    "\n",
    "Cerain characters have special meaning in regular expressions. \n",
    "\n",
    "[] - A pair of brackets is used to indicate a set of characters.  \n",
    "'\\' - Either escapes special character or signals a special sequence.  \n",
    "?\t- The question mark indicates there is zero or one of the preceding   \n",
    "'*'\t- The asterisk indicates there is zero or more of the preceding element  . \n",
    "+\tThe plus sign indicates there is one or more of the preceding element.  \n",
    "'^' (Caret.) Matches the start of the string.    \n",
    "'$' Matches the end of the string.  \n",
    "{m} (Braces) Specifies that exactly m copies of the previous RE should be matched. \n",
    "\n",
    "\n",
    "For example, the regexp $[A-Za-z]+$ matches a seuqnece of at least one upper or lower case letters.  The regexp $^[ ]+A-Za-z0-9._-]+@[[A-Za-z0-9.-]+$[ ]+$ matches an e-mail pattern with starting and ending white spaces. There are many excellent books that describe regular expressions in detail.   \n",
    "\n",
    "## Term-Document matrices  \n",
    "\n",
    "A [term-document matrix](https://en.wikipedia.org/wiki/Document-term_matrix) is a matrix where the rows correspond to documents in the collection and columns correspond to terms. Creating term-document matrices are created using the [R text mining package tm](https://cran.r-project.org/web/packages/tm/index.html).  \n",
    "\n",
    "D1 = \"I love R\"   \n",
    "D2 = \"I love ice-cream\"   \n",
    "\n",
    "then the document-term matrix would be:\n",
    "\n",
    "$$\n",
    "TermDocument=\n",
    "  \\begin{bmatrix}\n",
    "      & I & love & R & ice-cream \\\\  \n",
    "    D1 & 1 & 1 & 1 & 0 \\\\\n",
    "    D2 & 1 & 1 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which shows which documents contain which terms and how many times they appear.\n",
    "\n",
    "## Frequency signatures\n",
    "\n",
    "For processing larger amounts of text, tag/word counts can be ineffecient using term-document matrices as these are typically very sparse matrices. Especially when one has a large dictionary of tags/words.  \n",
    "\n",
    "For larger data we used a “frequency signature” approach to convert a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) output to a format that we can use to calculate tag co-occurrence associations and mutual information. Frequency signatures are described in detail in Stefan Evert’s PhD dissertation “The Statistics of Word Cooccurrences Word Pairs and Collocations.”   \n",
    "\n",
    "\n",
    "To calculate tag co-occurrence associations and mutual information for two tags, A and B, we need four items of data. The co-occurrence count of A and B, the count of A but not B, the count of B but not A, and the total number of tags in a corpus. This co-occurrence frequency data for a word pair (A,B) are usually organized in a contingency table show below. The contingency table stores the observed frequencies $O_{11} … O_{22}$. The table below (adapted from Evert’s dissertation) shows an observed contingency table.\n",
    "\n",
    "Contingency Tables\n",
    "\n",
    "![Contingency Tables](http://nikbearbrown.com/YouTube/MachineLearning/M08/Contingency_Tables.png)  \n",
    "\n",
    "*Contingency table : $O_{11}$ is co-occurrence count of A and B, $O_{12}$ is the count of A but not B, $O_{21}$  is the count of B but not A, and $O_{22}$ is the count of not B and not A.*  \n",
    "\n",
    "\n",
    "However, while the co-occurrence count of A and B, and the total number of tags in a corpus are efficiently and easily counted the count of A but not B, the count of B but not A are tricky and computationally expensive. The insight and advantage of frequency signatures is that they calculate the count of A but not B, the count of B but not A by just counting A and B and the co-occurrence count of A and B. That is, the count of A but not B is equal to count of A minus the co-occurrence count of A and B. Likewise, the count of B but not A is equal to count of B minus the co-occurrence count of A and B.   \n",
    "\n",
    "The frequency signature of a tag pair (A, B) is usually written as $(f, f_1, f_2,N)$. Where $f$ is the co-occurrence count of A and B, $f_1$ is the count of A but not B, $f_2$ is the count of B but not A, and N is the total counts. Notice that the observed frequencies $O_{11}, ..., O_{22}$ can be directly calculated from the frequency signature by the equations below:  \n",
    "\n",
    "* $O_{11} = f$   \n",
    "* $O_{12} = f_1 − f$   \n",
    "* $O_{21} = f_2 – f$   \n",
    "* $O_{22} = N − f_1 − f_2 + f$   \n",
    "\n",
    "Generating all of the data tag co-occurrence association and mutual information calculations using this approach can be generated using a single pass of the data and two associative arrays; one of the tag counts and another for the tag co-occurrence counts.\n",
    "\n",
    "Calculating Associations and Mutual Information from Frequency Signatures\n",
    "\n",
    "Evert shows the many association and mutual information statistics can be calculated from the observed frequencies $O_{11}, ..., O_{22}$ if we can generate the expected frequencies $E_{11}, ..., E_{22}$. The table below (adapted from Evert’s dissertation) shows the expected versus observed contingency tables.  \n",
    "\n",
    "![Frequency Signatures](http://nikbearbrown.com/YouTube/MachineLearning/M08/Frequency_Signatures.png)  \n",
    "*Frequency Signatures*  \n",
    "  \n",
    "The sum of all four observed frequencies (called the sample size N) is equal to the total number of pair tokens extracted from the corpus. R1 and R2 are the row totals of the observed contingency table, while C1 and C2 are the corresponding column totals. The expected frequencies can be directly calculated from observed frequencies $O_{11}, ..., O_{22}$ by the equations below:  \n",
    "\n",
    "* $R1 = O_{11} + O_{12}$    \n",
    "* $R2 = O_{21} + O_{22}$    \n",
    "* $C1 = O_{11} + O_{21}$   \n",
    "* $C2 = O_{12} + O_{22}$   \n",
    "* $N = O_{11} + O_{12} + O_{12} + O_{22}$   \n",
    "\n",
    "\n",
    "Evert went on to show that several association measures can be easily calculated once one has the expected and observed contingency tables. For example, the pointwise mutual information (MI) is calculated by below.\n",
    "\n",
    "$pointwise \\quad mutual \\quad information \\quad MI=\\ln\\frac{O_{11}}{E_{11}}$\n",
    "\n",
    "The Likelihood measures that can be calculated using the expected and observed contingency tables are:  \n",
    "\n",
    "* Multinomial-likelihood   \n",
    "* Binomial-likelihood  \n",
    "* Poisson-likelihood   \n",
    "* Poisson-Stirling approximation    \n",
    "* Hypergeometric-likelihood    \n",
    "\n",
    "The exact hypothesis tests that can be calculated using the expected and observed contingency tables are:   \n",
    "\n",
    "* binomial test    \n",
    "* Poisson test\n",
    "* Fisher's exact test   \n",
    "\n",
    "The asymptotic hypothesis tests that can be calculated using the expected and observed contingency tables are:  \n",
    "\n",
    "* z-score    \n",
    "* Yates' continuity correction   \n",
    "* t-score (which compares O11 and E11 as random variates)    * Pearson's chi-squared test   \n",
    "* Dunning's log-likelihood (a likelihood ratio test)   \n",
    "\n",
    "The measures from information theory that can be calculated using the expected and observed contingency tables are:    \n",
    "\n",
    "* MI (mutual information, mu-value)   \n",
    "* logarithmic odds-ratio logarithmic relative-risk    \n",
    "* Liddell's difference of proportions   \n",
    "* MS (minimum sensitivity)   \n",
    "* gmean (geometric mean) coefficient   \n",
    "* Dice coefficient (aka. \"mutual expectation\")   \n",
    "* Jaccard coefficient   \n",
    "* MIconf (a confidence-interval estimate for the mu-value)   \n",
    "* MI (pointwise mutual information)   \n",
    "* local-MI (contribution to average MI of all co-occurrences)   \n",
    "* average-MI (average MI between indicator variables)   \n",
    "\n",
    "Stefan Evert also developed a R library called [UCS toolkit](http://www.collocations.de/software.html) for the statistical analysis of co-occurrence data with association measures and their evaluation in a collocation extraction task.    \n",
    "\n",
    "\n",
    "## tf–idf\n",
    "\n",
    "[Tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) or term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document.\n",
    "\n",
    "The tf-idf value increases proportionally to the number of times a word/tag appears in the document, but is offset by the frequency of the word/tag. It is a measure of jargon. If a word appears frequently in a document, it's important. Give the word a high score. But if a word appears in every documents, it's not specifc to a topic. Give the word a low score.  \n",
    "\n",
    "If we want to find jargon, that is, topic or subject specifc words/tags then this is a reasonable metric. Words like 'the', 'a', get low scores as the are in every document. While words/tags like 'Machine Learning', 'Twitter,' or 'Text Mining' get high scores since they are used a lot in specific contexts.  \n",
    "\n",
    "If with call $f(t,d)$ the raw frequency of a term in a document, i.e. the number of times that term t occurs in document d and $max(f(t,d))$ the maximum raw frequency of any term in the document, then the term frequency $tf(t,d)$ is:\n",
    "\n",
    "$$\n",
    "\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(t, d):t \\in d\\}}\n",
    "$$\n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, It is the logarithmically scaled fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n",
    "\n",
    "$$\n",
    " \\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}\n",
    "$$\n",
    "\n",
    "with  \n",
    "\n",
    "* $N$: total number of documents in the corpus $N = {|D|}$   \n",
    "* $|\\{d \\in D: t \\in d\\}|$ : number of documents where the term  t  appears (i.e.,  $\\mathrm{tf}(t,d) \\neq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to add a single \"pseudocount.\"\n",
    "\n",
    "Mathematically the base of the log function does not matter for these purposes.  \n",
    "\n",
    "Then tf–idf is calculated as\n",
    "\n",
    "$$\\mathrm{tf-idf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$\n",
    "\n",
    "A high weight in tf–idf means a tag/word has high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents.   \n",
    "\n",
    "For how this is done in R see [The tf-idf-Statistic For Keyword Extraction](http://www.r-bloggers.com/the-tf-idf-statistic-for-keyword-extraction/)\n",
    "\n",
    "## Word entropy and entropy rate\n",
    "\n",
    "In information theory, entropy is also a measure of the uncertainty in a random variable. Like tf–idf, entropy quantifies the expected value of the information contained in a message (or the expected value of the information of the probability distribution). Typically this is expressed in the number of ‘bits’ or ‘nats’ that are required to encode a given message. \n",
    "\n",
    "In this sense entropy can be used to estimate (like tf–idf) how much information is in a word or tag. Entropy can also be used to estimate the generating probability distribution for a text document or corpus. The entropy of many languages has been determined. English has 1.65 bits per word, French has 3.02 bits per word, German has 1.08 bits per word, and Spanish has 1.97 bits per word. Given the probability density function of word entropies and the average bits per word of a single tweet we could then assign probabilities that it is English, French, German, or Spanish.\n",
    "\n",
    "\n",
    "### Entropy rate\n",
    "\n",
    "The entropy rate (or mean entropy rate) describes the limiting entropy over an entire probability distribution.  This can be thought of as the average entropy over a sufficiently long realization of a stochastic process, whereas the entropy is relevant to a single random variable at a given point in time.  \n",
    "\n",
    "In statistics, ergodicity describes a random process wherein the average time for one sufficiently long realization of events is the same as the ensemble average. That is, the ensemble’s statistical properties (such as its mean or entropy) can be deduced from a single, sufficiently long sample of the process. In other words, there are long-term invariant measures that describe the asymptotic properties of the underlying probability distribution, and they can be measured by following any single reprehensive portion if followed long enough. For example, if I look at two particles in an ergodic system at any time, those particles may have very different states; but if I follow those particles long enough, they become statistically indistinguishable from one another. This means that statistical properties of the entire system can be deduced from a single sample of the process if followed for a sufficiently long time.   \n",
    "\n",
    "Stationarity is the property of a random process which guarantees that the aggregate statistical properties of the probability density function, such as the mean value, its moments and variance, remain the same at every point in time. A stationary process, therefore, is one whose probability distribution is the same at all times. Its statistical properties cannot necessarily be deduced from a single sample of the process. There are stochastic processes that exhibit both stationarity and ergodicity called stationary ergodic processes. These are random processes that will not change their statistical properties with time; hence, the properties, including the disorder (entropy) of the system, can be deduced from a single, sufficiently long sample realization of the process. There are weaker forms of the stationary condition in which the first- and second-order moments (that is, the mean and variance) of a stochastic process are constant but other properties of the probability density function can vary. Likewise, there are stationary stochastic processes that are not themselves ergodic but are composed of a mixture of ergodic components.\n",
    "\n",
    "##### Rényi entropies  \n",
    "\n",
    "The [Rényi entropies](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy) generalize the Shannon entropy, the Hartley entropy, the min-entropy, and the collision entropy. As such, these entropies as an ensemble are often called the Rényi entropies (or the Rényi entropy, even though this usually refers to a class of entropies). The difference between these entropies is in the respective value for each of an order parameter called alpha: the values of alpha are greater than or equal to zero but cannot equal one. The Renyi entropy ordering is related to the underlying probability distributions and allows more probable events to be weighted more heavily. As alpha approaches zero, the Rényi entropy increasingly weighs all possible events more equally, regardless of their probabilities. A higher alpha (a) weighs more probable events more heavily. The base used to calculate entropies is usually base 2 or Euler's number base e. If the base of the logarithm is 2, then the uncertainty is measured in bits. If it is the natural logarithm, then the unit is nats. \n",
    "\n",
    "##### Rényi entropies\t \n",
    "\n",
    "The Rényi entropy of order $\\alpha$, where $\\alpha \\geq 0$  and $\\alpha \\neq 1$ , is defined as\n",
    "\n",
    "$$\n",
    "H_\\alpha(X) = \\frac{1}{1-\\alpha}\\log\\Bigg(\\sum_{i=1}^n p_i^\\alpha\\Bigg)\n",
    "$$\n",
    "\n",
    "Here, X is a discrete random variable with possible outcomes 1,2,...,n and corresponding probabilities $p_i \\doteq \\Pr(X=i) for i=1,\\dots,n,$ and the logarithm is base 2. \n",
    "\n",
    "\n",
    "#### Hartley entropy\n",
    "\n",
    "The Hartley entropy (Gray, 1990) is the Rényi entropy with an alpha of zero. \n",
    "\n",
    "the probabilities are nonzero, $H_0$ is the logarithm of the cardinality of X, sometimes called the Hartley entropy of X:  \n",
    "\n",
    "$$\n",
    "H_0 (X) = \\log n = \\log |X|\n",
    "$$\n",
    "\n",
    "#### Shannon entropy \n",
    "\n",
    "The Shannon entropy (Gray, 1990) is the Rényi entropy with an alpha of one. The Shannon entropy is a simple estimate of the expected value of the information contained in a message. It assumes independence and identically distributed random variables, which is a simplification when applied to word counts. In this sense it is analogous to naïve Bayes, in that it is very commonly used and thought to work well in spite of violating some assumptions upon which it is based.\n",
    "\n",
    "The limiting value of $H_\\alpha as \\alpha \\rightarrow 1$ is the Shannon entropy:\n",
    "\n",
    "$$\n",
    "H_1 (X) = - \\sum_{i=1}^n p_i \\log p_i. \n",
    "$$\n",
    "\n",
    "#### collision entropy\n",
    "\n",
    "The collision entropy (Gray, 1990) is the Rényi entropy with an alpha of two and is sometimes just called \"Rényi entropy,\" refers to the case $\\alpha = 2$,\n",
    "\n",
    "$$\n",
    "H_2 (X) = - \\log \\sum_{i=1}^n p_i^2 = - \\log P(X = Y)\n",
    "$$\n",
    "\n",
    "where $X$ and $Y$ are independent and identically distributed. \n",
    "\n",
    "#### min-entropy\n",
    "\n",
    "The min-entropy (Gray, 1990) is the Rényi entropy as the limit of alpha approaches infinity. The name min-entropy stems from the fact that it is the smallest entropy measure in the Rényi family of entropies. In the limit as $\\alpha \\rightarrow \\infty$, the Rényi entropy $H_\\alpha converges to the min-entropy H_\\infty$:\n",
    "\n",
    "$$\n",
    "H_\\infty(X) \\doteq \\min_i (-\\log p_i) = -(\\max_i \\log p_i) = -\\log \\max_i p_i\\,.\n",
    "$$\n",
    "\n",
    "Equivalently, the min-entropy $H_\\infty(X)$ is the largest real number b such that all events occur with probability at most $2^{-b}$.\n",
    "\n",
    "\n",
    "#### Kullback-Leibler divergence\n",
    "\n",
    "[Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (Gray, 1990) is a non-symmetric measure of the difference between two probability distributions. The Kullback-Leibler measure goes by several names: relative entropy, discrimination information, Kullback-Leibler (KL) number, directed divergence, informational divergence, and cross entropy. Kullback-Leibler divergence is a measure of the difference between the observed entropy and its excepted entropy. We calculate the KL divergence by weighting one distribution (like an observed frequency distribution) by the log of probabilities of some other distribution D2. For discrete probability distributions P and Q, the Kullback–Leibler divergence of Q from P is defined to be\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P\\|Q) = \\sum_i P(i) \\, \\ln\\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "In words, it is the expectation of the logarithmic difference between the probabilities P and Q, where the expectation is taken using the probabilities P.\n",
    "\n",
    "\n",
    "#### Mutual Information\n",
    "\n",
    "[Mutual information](https://en.wikipedia.org/wiki/Mutual_information) (Gray, 1990) quantifies the mutual dependence of the two random variables. It is a measure of the “stickiness” between two items. It measures how much knowing one of these variables reduces uncertainty about the other. We can use mutual information to quantify the association between two tags. Mutual information (Equation 10) is given by:\n",
    "\n",
    "the mutual information of two discrete random variables X and Y can be defined as:\n",
    "\n",
    "$$\n",
    " I(X;Y) = \\sum_{y \\in Y} \\sum_{x \\in X} \n",
    "                 p(x,y) \\log{ \\left(\\frac{p(x,y)}{p(x)\\,p(y)}\n",
    "                              \\right) }, \\,\\!\n",
    "$$                              \n",
    "                              \n",
    "where $p(x,y)$ is the joint probability distribution function of $X$ and $Y$, and $p(x)$ and $p(y)$ are the marginal probability distribution functions of $X$ and $Y$ respectively. In the case of continuous random variables, the summation is replaced by a definite double integral:\n",
    "\n",
    "$$\n",
    " I(X;Y) = \\int_Y \\int_X \n",
    "                 p(x,y) \\log{ \\left(\\frac{p(x,y)}{p(x)\\,p(y)}\n",
    "                              \\right) } \\; dx \\,dy,\n",
    "$$\n",
    " \n",
    "where $p(x,y)$ is now the joint probability density function of $X$ and $Y$, and $p(x$) and $p(y)$ are the marginal probability density functions of $X$ and $Y$ respectively.\n",
    "\n",
    "\n",
    "\n",
    "## Zipf's law\n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) is an empirical law formulated using mathematical statistics, refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. It states that while only a few words are used very often, many or most are used rarely, \n",
    "\n",
    "$P_n \\sim 1/n^a$\n",
    "\n",
    "where $P_n$ is the frequency of a word ranked nth and the exponent a is almost 1. This means that the second item occurs approximately 1/2 as often as the first, and the third item 1/3 as often as the first, and so on. The is a so-called \"power law\" distributon.  \n",
    "\n",
    "The law is very common when looking a the distributions of words in all lagnuages. In fact, it is named after the American linguist [George Kingsley Zipf](https://en.wikipedia.org/wiki/George_Kingsley_Zipf) when he observed this law in 1935 in his academic studies of word frequency.  \n",
    "\n",
    "## Text-mining pipeline\n",
    "\n",
    "Common Natural language processing tasks, such as tokenization,  stemming, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, word disamiguation, summarization, and coreference resolution are often done on every new text document before other types of analysis. As such it is common to write a \"pipeline\" (i.e. a sequence of scripts) that automatically perform these tasks.\n",
    "\n",
    "\n",
    "*Tokenization*  \n",
    "\n",
    "[Tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) is the task of chopping it up into pieces, called tokens. This is often done by throwing away certain characters, such as punctuation.  \n",
    "\n",
    "*Stemming*\n",
    "\n",
    "[Stemming](https://en.wikipedia.org/wiki/Stemming) reduces words to their word stem, base or root form. For example, \"argue\", \"argued\", \"argues\", \"arguing\", and \"argus\" reduce to the stem \"argu.\"  The can allow for \"argue\", \"argued\", \"argues\" to be counted as a single word.  \n",
    "\n",
    "*Sentence segmentation*  \n",
    "\n",
    "[sentence segmentation](http://www.monlp.com/2012/03/13/segmenting-words-and-sentences/) is the process plitting text into words and sentences. This is often done by looking for certain punctuation (full stop, question mark, exclamation mark, etc.).  \n",
    "\n",
    "*Part-of-speech tagging*  \n",
    "\n",
    "Given some text [part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging) determines the [part of speech](https://en.wikipedia.org/wiki/Part_of_speech) for each word. (e.g. a noun, verb, adjective. etc.)\n",
    "\n",
    "\n",
    "*Named entity extraction*\n",
    "\n",
    "[Named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) determines which items in the text map to proper nouns, such as people or places, or things.\n",
    "\n",
    "*Chunking* \n",
    "\n",
    "Chunking is also called shallow parsing. It is the identification short meaningful n-grams (like noun phrases, named entities, collocations, etc.).  In this processing of a tweet, it could refer to sperating the hashtags, the links and the tweet.  \n",
    "\n",
    "*Parsing*\n",
    "\n",
    "[Parsing](https://en.wikipedia.org/wiki/Parsing) or syntactic analysis generates [https://en.wikipedia.org/wiki/Parsing] the parse tree (grammatical analysis) for each sentence or fragment. \n",
    "\n",
    "*Word-sense disambiguation*  \n",
    "\n",
    "Many words have more than one meaning. [Word-sense disambiguation](https://en.wikipedia.org/wiki/Word-sense_disambiguation] selectx the meaning which makes the most sense in context.\n",
    "\n",
    "*Coreference resolution*\n",
    "\n",
    "Given a sentence or larger chunk of text, [coreference resolution](https://en.wikipedia.org/wiki/Coreference) determines which words (\"mentions\") refer to the same objects (\"entities\")\n",
    "\n",
    "*Automatic summarization*\n",
    "\n",
    "Automatic summarization produce a readable summary of a chunk of text. The summary need not be grammatical, for example, a [tag cloud](https://en.wikipedia.org/wiki/Tag_cloud) could be thought of as a visual summarization of some text.  \n",
    "\n",
    "\n",
    "Which elements are part of a Text-mining pipeline depend on the application.  \n",
    "\n",
    "\n",
    "## Small Talk and the Social Web  \n",
    "\n",
    "One particularly interesting aspect of the social web is the nature of text made available for public consumption. From the time of the Gutenberg printing press until the advent of Web 2.0, nearly all text presented in public was written by professionals. Whether it was a book, a business or government record, a sermon, a news or opinion article, a scientific paper, or an advertisement, it was written by a professional with the intent to communicate information and/or ideas. Not until the social web and Twitter did musings about what a noncelebrity ate for breakfast or whether someone likes naps was widely available for public consumption.    \n",
    "\n",
    "Musings about one’s foot fungus or a statement like “ Hahahahahaha!!!! You should have come to NKLA!!! So many beautiful pitties! And pittie lovers....”  are what we call small talk. Small talk is light, intimate banter, often understandable only by the authors’ close friends. A lot of the communication on the social web is small talk, even though it was very rare in public writing prior to the social web.  \n",
    "\n",
    "This free very unstructured nature of language on the social web should be taken into account when mining these data. It can add a considerable amount of noise and magnifies the importance of good semantic and lexcial resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ensemble methods  \n",
    "\n",
    "In statistics and machine learning, [ensemble methods](https://en.wikipedia.org/wiki/Ensemble_learning) use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms.\n",
    "\n",
    "Ensemble methods groups a collection of base learners, with each learning according to its arget function. The ensemble then combines their outputs for a final predication. The basic idea is \"two heads are better than one.\n",
    "\" (assuming those heads make independent decsisons). This is sometimes called “meta-learning”.\n",
    "\n",
    "![Ensemble learning mixes multiple approaches to obtain better predictive performance.](http://nikbearbrown.com/YouTube/MachineLearning/M09/Emsemble_Learning.png)    \n",
    "*Ensemble Learning*\n",
    "\n",
    "The basic issue with ensemble learning is:    \n",
    "* How can you choose independent learners?    \n",
    "* How can you combine learners?    \n",
    "\n",
    "## Ensemble theory\n",
    "\n",
    "Ensemble learning build many models and combines them. ensemble methods can be shown both theoretically and empirically to outperform single predictors on a wide range of tasks.\n",
    "\n",
    "*When does this work?*\n",
    "\n",
    "* Let p be the probability that a classifier makes an error  \n",
    "* Assume that classifier errors are independent    \n",
    "* The probability that k of the n classifiers make an error    \n",
    "\n",
    "$$\n",
    "\\binom n k  p^k(1-p)^{n-k}\n",
    "$$ \n",
    " \n",
    "for k = 0, 1, 2, ..., n, where\n",
    "$$\n",
    "\\binom n k =\\frac{n!}{k!(n-k)!}\n",
    "$$\n",
    "is the binomial coefficient, hence the name of the distribution.\n",
    "\n",
    "Therefore the probability that a majority vote classifier is in error is:   \n",
    "\n",
    "$$\n",
    " \\sum_{k>n/2}^{n} \\binom n k  p^k(1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "\n",
    "### Majority Vote Model\n",
    "\n",
    "In the majority vote model the ensemble chooses the class predicted by more than $n/2$ the $n$ classifiers. If no agreement then return an error.\n",
    "\n",
    "\n",
    "### When is an Ensemble Better than a Single Learner?\n",
    "\n",
    "*“No Free Lunch” Theorem*  \n",
    "\n",
    "No single algorithm wins all the time! By combing multiple independent decisions, each of which is at least more accurate than random guessing, random errors cancel each other out and correct decisions are reinforced. Empirically, ensembles tend to yield better results when there is a significant diversity among the models. \n",
    "\n",
    "*Bias Problem*   \n",
    "\n",
    "The hypothesis space made available by a particular classification method does not  include the true hypothesis. The bias is error from erroneous assumptions in the learning algorithm.   \n",
    "\n",
    "*Variance Problem*  \n",
    "\n",
    "The hypothesis space is “too large” for the amount of training data – thus selected hypothesis may be inaccurate on unseen data. The variance is error from sensitivity to small fluctuations in the training set. High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.   \n",
    "\n",
    "Bias and variance usually work in opposition to one other: attempts to reduce the bias component will cause an increase in variance, and vice versa. Many ensemble methods, therefore, seek to promote diversity among the models they combine. Ensembles methods tend to decrease error by decreasing the variance in the results due to unstable learners, algorithms  whose output can change dramatically when the training data is slightly changed. However, these random errors may not cancel and be further reinfornced if the learners all have the same bias.\n",
    "\n",
    "### Condorcet Jury Theorem\n",
    "\n",
    "[Condorcet's jury theorem](https://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem) is a political science theorem about the relative probability of a given group of individuals arriving at a correct decision. The theorem was first expressed by the Marquis de Condorcet in his 1785 work Essay on the Application of Analysis to the Probability of Majority Decisions.\n",
    "\n",
    "If each voter has an independent probability p of voting for the correct decision then the critical p for which majority voting works is a probability greater than or less than 1/2:   \n",
    "\n",
    "* If p is greater than 1/2 (each voter is more likely to vote correctly), then adding more voters increases the probability that the majority decision is correct. In the limit, the probability that the majority votes correctly approaches 1 as the number of voters increases.   \n",
    "\n",
    "* On the other hand, if p is less than 1/2 (each voter is more likely to vote incorrectly), then adding more voters makes things worse: the optimal jury consists of a single voter.   \n",
    "\n",
    "* It is critical that voter has an independent probability.   \n",
    "\n",
    "The probability of a correct majority decision P(n,p) (p close to 1/2), the gain by having n voters grows proportionally to \\sqrt{n}.\n",
    "\n",
    "$$  \n",
    " P(n,p) = 1/2 + c_1 (p-1/2) + c_3 (p-1/2)^3 + O( (p-1/2)^5 ) \n",
    "$$   \n",
    "where\n",
    "$$  \n",
    " c_1 = {n  \\choose { \\lfloor n/2 \\rfloor}} \\frac{ \\lfloor n/2 \\rfloor +1} { 4^{\\lfloor n/2 \\rfloor}} = \\sqrt{ \\frac{2n+1}{\\pi}} (1 + \\frac{1}{16n^2} + O(n^{-3}) ) \n",
    "$$  \n",
    "\n",
    "### Wisdom of Crowds \n",
    "\n",
    "The \"Wisdom of Crowds\" refers to situations in which human ensembles are demonstrably better than individual experts. This has often been observed in situations like:\n",
    "\n",
    "* How many jelly beans are in the jar?   \n",
    "* Who Wants to be a Millionaire:  “Ask the audience”\n",
    "* Accurately guessing the weight of an ox\n",
    "\n",
    "Many examples are given in [James Surowiecki's](https://en.wikipedia.org/wiki/James_Surowiecki) book The [Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations](https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds), published in 2004.\n",
    "\n",
    "Surowiecki notes that Not all crowds (groups) are wise.  Charles Mackay's [Extraordinary Popular Delusions and the Madness of Crowds](https://en.wikipedia.org/wiki/Extraordinary_Popular_Delusions_and_the_Madness_of_Crowds), published in 1841, is a history of popular crowd delusions. Rather he states four key criteria separate wise crowds from irrational ones:\n",
    "\n",
    "* Diversity of opinion - Each person should have private information even if it's just an eccentric interpretation of the known facts.  \n",
    "* Independence - People's opinions aren't determined by the opinions of those around them.  \n",
    "* Decentralization - People are able to specialize and draw on local knowledge.   \n",
    "* Aggregation\tSome mechanism exists for turning private judgments into a collective decision. \n",
    "\n",
    "For ensemble learners certainly the following are critical \n",
    "\n",
    "* Independence\n",
    "* Aggregation\n",
    "\n",
    "and the following certainly helps\n",
    "\n",
    "* Diversity of learners\n",
    "\n",
    "It's not so clear that ensemble learners should specialize and draw on local knowledge; but the effect is related to diversity.    \n",
    "\n",
    "\n",
    "## Common types of ensembles\n",
    "\n",
    "### Bootstrap aggregating (bagging)\n",
    "\n",
    "Create ensembles by [“bootstrap aggregation”](https://en.wikipedia.org/wiki/Bootstrap_aggregating), i.e., repeatedly randomly re-sampling training data. Not that bagging uses the same learner so bias related to the method isn't addressed by this approach. \n",
    "\n",
    "Bootstrap: draw n items from X with replacement\n",
    "\n",
    "Bootstrap aggregating: combines random learners (often with voting, averaging or median) to create a predictor lesss efected by noise. Unstable and/or noisy algorithms often profit from bagging.\n",
    "\n",
    "Bagging's usefulness depends on the stability of the base classifiers.  If small changes in the sample cause small changes in the base-level classifier, then the ensemble will not be much better than the base classifiers. It reduces variance and helps to avoid overfitting. It is often applied to decision tree methods (random forests) and nearest neighbor classifiers, but it can be used with any type of method.  \n",
    "\n",
    "### Boosting\n",
    "\n",
    "Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In these sense it \"learns.\" Unlike bagging, weights may change at the end of boosting round making certain learners more important than others. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends tp propgate bias from the overweighting winning predictor and is more likely to over-fit the training data. By far, the most common implementation of Boosting is Adaboost.\n",
    "\n",
    "*Summary of Boosting and Bagging*\n",
    "\n",
    "* Called “homogenous ensembles” becuase there is a single learner \n",
    "* Bagging: Resample training data\n",
    "* Boosting: Reweight training data\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost), short for \"Adaptive Boosting.\" AdaBoost refers to a particular method of training a boosted classifier. A boost classifier is a classifier in the form\n",
    "\n",
    "$$\n",
    "F_T(x) = \\sum_{t=1}^T f_t(x)\\,\\!\n",
    "$$\n",
    "\n",
    "where each $f_t$ is a weak learner that takes an object $x$ as input and returns a real valued result indicating the class of the object. \n",
    "\n",
    "Each weak learner produces an output, hypothesis $h(x_i)$, for each sample in the training set. At each iteration t, a weak learner is selected and assigned a coefficient $\\alpha_t$ such that the sum training error $E_t$ of the resulting t-stage boost classifier training error is minimized.    \n",
    "\n",
    "$E_t = \\sum_i E[F_{t-1}(x_i) + \\alpha_t h(x_i)]$\n",
    "Here $F_{t-1}(x)$ is the boosted classifier that has been built up to the previous stage of training, $E(F)$ is some error function and $f_t(x) = \\alpha_t h(x)$ is the weak learner that is being considered for addition to the final classifier.\n",
    "\n",
    "AdaBoost can be thought of as a form of gradient descent along the error gradient.   \n",
    "\n",
    "\n",
    "### Bayesian parameter averaging\n",
    "\n",
    "Bayesian classifiers are determined by the parameters of the models. Bayesian parameter averagingis an ensemble of all the hypotheses in the hypothesis space. The idea is that the bias of the modelds does not change, but variance decreases by the number of independent models.   \n",
    "\n",
    "\n",
    "### Bucket of models\n",
    "\n",
    "A \"bucket of models\" is an ensemble in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.\n",
    "\n",
    "```\n",
    "For each model m in the bucket:\n",
    "  Do c times: (where 'c' is some constant)\n",
    "    Randomly divide the training dataset into two datasets: A, and B.\n",
    "    Train m with A\n",
    "    Test m with B\n",
    "Select the model that obtains the highest average score\n",
    "```\n",
    "This idea can be summed up as, \"try them all with the training set, and pick the one that works best\".\n",
    "\n",
    "\n",
    "### Stacking\n",
    "\n",
    "Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms.\n",
    "\n",
    "Unlike a bucket of models it isn't a \"winner take all\" approach. Unlike “homogenous ensembles” like bagging and boosting here we are creating “heterogenous ensembles” and various preictors.     \n",
    "\n",
    "### Random Forests\n",
    "\n",
    "[Random forests](https://en.wikipedia.org/wiki/Random_forest) involve bagging multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.  \n",
    "\n",
    "```\n",
    "For i = 1 to T,\n",
    "  Take a bootstrap sample (bag)\n",
    "  Grow a random decision tree  T_i\n",
    "  At each node choose a feature from one of n features (n < total number of features)\n",
    "  Grow a full tree (do not prune)\n",
    "\n",
    "Final classification is done by majority vote across the T random trees.\n",
    "```\n",
    "\n",
    "## Developing Ensemble Models?\n",
    "\n",
    "The main challenge is not to obtain highly accurate base models, very weak predictors with an accuracy of greater than 50% can be used, but rather to obtain base models which make different kinds of errors.   \n",
    "\n",
    "When one resamples or rewights training data, as one does with bagging amd boosting it helps with noisy data but not the predictor itself.\n",
    "\n",
    "To generate “heterogenous ensembles” independence between the base classifiers needs to be assessed. Independence between classifiers is often assed by measuring the degree of overlap in misclassifying training examples. Less overlap means more independence between models. The key of designing ensembles is diversity and not necessarily high accuracy of the base classifiers: Members of the ensemble should vary in the examples they misclassify. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    "## Probability and Statistics\n",
    "\n",
    "Probability is a measure of the likelihood of a random phenomenon or chance behavior.  Probability describes the long-term proportion with which a certain outcome will occur in situations with short-term uncertainty. \n",
    "\n",
    "### The Axioms of Probability\n",
    "\n",
    "\n",
    "##### First axiom - The probability of an event is a non-negative real number:\n",
    "$$\n",
    "P(E)\\in\\mathbb{R}, P(E)\\geq 0 \\qquad \\forall E\\in F\n",
    "$$\n",
    "where $F$ is the event space\n",
    "\n",
    "##### Second axiom -  unit measure:\n",
    "\n",
    "The probability that some elementary event in the entire sample space will occur is 1.\n",
    "\n",
    "$$\n",
    "P(\\Omega) = 1.\n",
    "$$  \n",
    "\n",
    "##### Third axiom - the assumption of $\\sigma$-additivity:\n",
    "\n",
    "Any countable sequence of disjoint (synonymous with mutually exclusive) events $E_1, E_2, ...$ satisfies\n",
    "\n",
    "$$\n",
    "P\\left(\\bigcup_{i = 1}^\\infty E_i\\right) = \\sum_{i=1}^\\infty P(E_i).\n",
    "$$\n",
    "\n",
    "The total probability of all possible event always sums to 1. \n",
    "\n",
    "### Consequences of these axioms\n",
    "The probability of the empty set:\n",
    "$$\n",
    "P(\\varnothing)=0.\n",
    "$$\n",
    "\n",
    "Monotonicity   \n",
    "$$\n",
    "\\quad\\text{if}\\quad A\\subseteq B\\quad\\text{then}\\quad P(A)\\leq P(B).\n",
    "$$\n",
    "\n",
    "The numeric bound between 0 and 1:  \n",
    "\n",
    "$$\n",
    "0\\leq P(E)\\leq 1\\qquad \\forall E\\in F.\n",
    "$$\n",
    "\n",
    "\n",
    "![Probability is expressed in numbers between 0 and 1](http://nikbearbrown.com/YouTube/MachineLearning/M10/Probability_0_1.png)    \n",
    "*Probability is expressed in numbers between 0 and 1.*   \n",
    "\n",
    "\n",
    "Probabilty of a certain event is 1:\n",
    "\n",
    "$$\n",
    "P(True) = 1\n",
    "$$\n",
    "\n",
    "Probability = 1 means it always happens.\n",
    "\n",
    "\n",
    "Probabilty of an impossible event is 0:\n",
    "\n",
    "$$\n",
    "P(False) = 0\n",
    "$$\n",
    "\n",
    "Probability = 0 means the event never happens.  \n",
    "\n",
    "Probabilty of A or B:\n",
    "\n",
    "$$\n",
    "P(A \\quad or \\quad B) = P(A) + P(A) - P(A \\quad and \\quad B) \n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "P(A \\cup B) = P(A) + P(A) - P(A \\cap B) \n",
    "$$\n",
    "\n",
    "\n",
    "Probabilty of not A:\n",
    "\n",
    "$$\n",
    "P(not  \\quad A) = 1- P(A) \n",
    "$$\n",
    "\n",
    "\n",
    "## Conditional Probability\n",
    "\n",
    "In probability theory, a [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability) measures the probability of an event given that another event has occurred. That is,  \"the conditional probability of A given B.\"   \n",
    "\n",
    " the conditional probability of A given B is defined as the quotient of the probability of the joint of events A and B, and the probability of B:\n",
    " \n",
    "$$ \n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "This may be visualized using a Venn diagram. \n",
    "\n",
    "![P(A and B) Venn](http://nikbearbrown.com/YouTube/MachineLearning/M10/Conditional_Probability_Venn_Diagram.png)     \n",
    "*$P(A \\cap B)$*\n",
    "\n",
    "### Corollary of Conditional Probability is The Chain Rule\n",
    "\n",
    "If we multiply both sides by $P(B)$ then\n",
    "\n",
    "$$ \n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "becomes\n",
    "\n",
    "$$ \n",
    "P(A|B) P(B) = P(A \\cap B) \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Statistical independence\n",
    "\n",
    "Events A and B are defined to be statistically independent if:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "             P(A \\cap B) &= P(A) P(B) \\\\\n",
    "  \\Leftrightarrow P(A|B) &= P(A) \\\\\n",
    "  \\Leftrightarrow P(B|A) &= P(B)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "That is, the occurrence of A does not affect the probability of B, and vice versa\n",
    "\n",
    "\n",
    "Probabilty of A or B for independent events $P(A and B) is 0$:\n",
    "\n",
    "$$\n",
    "P(A or B) = P(A) + P(A) \n",
    "$$\n",
    "\n",
    "## Bayes Rule\n",
    "\n",
    "[Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) (alternatively Bayes' law or Bayes' rule) describes the probability of an event, given prior events. That is, a conditional probability.\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A)\\, P(B | A)}{P(B)},\n",
    "$$\n",
    "\n",
    "where A and B are events.\n",
    "\n",
    "* P(A) and P(B) are the independent probabilities of A and B.  \n",
    "* P(A | B), a conditional probability, is the probability of observing event A given that B is true.  \n",
    "* P(B | A), is the probability of observing event B given that A is true.  \n",
    "\n",
    "\n",
    "## Bayesian inference\n",
    "\n",
    "[Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference) is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as evidence. Bayesian inference derives the posterior probability as a consequence of two antecedents, a prior probability and a \"likelihood function\" derived from a statistical model for the observed data. \n",
    "\n",
    "Bayesian inference computes the posterior probability according to Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(H\\mid E) = \\frac{P(E\\mid H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "where,    \n",
    "\n",
    "$P(H\\mid E)$ the posterior probability, denotes a conditional probability of $\\textstyle H$ (the hypothesis) whose probability may be affected by the evidence $\\textstyle E$.   \n",
    "\n",
    "$\\textstyle P(H)$, the prior probability, is an estimate of the probability that a hypothesis is true, before observing the current evidence.   \n",
    "\n",
    "$\\textstyle P(E\\mid H)$ is the probability of observing $\\textstyle E$ given $\\textstyle H$. It indicates the compatibility of the evidence with the given hypothesis.   \n",
    "\n",
    "$\\textstyle P(E)$ is sometimes termed the marginal likelihood or \"model evidence\". This factor is the same for all possible hypotheses being considered. \n",
    "\n",
    "Note that Bayes' rule can also be written as follows:\n",
    "$$\n",
    "P(H\\mid E) = \\frac{P(E\\mid H)}{P(E)} \\cdot P(H)\n",
    "$$\n",
    "where the factor $\\textstyle \\frac{P(E\\mid H)}{P(E)}$ represents the impact of $E$ on the probability of $H$.   \n",
    "\n",
    "## Bayesian probability example\n",
    "\n",
    "Suppose a certain disease has an incidence rate of 0.01% (that is, it afflicts 0.01% of the population).  A test has been devised to detect this disease.  The test does not produce false negatives (that is, anyone who has the disease will test positive for it), but the false positive rate is 1% (that is, about 1% of people who take the test will test positive, even though they do not have the disease).  Suppose a randomly selected person takes the test and tests positive.  What is the probability that this person actually has the disease?\n",
    "\n",
    "Bayes theorem would ask the question, what is the probability of disease given a postive result, or $P(disease\\mid positive))$. \n",
    "\n",
    "What do we know?  \n",
    "\n",
    "$P(positive\\mid disease)=1$ (i.e. The test does not produce false negatives.)     \n",
    "$P(disease)=0.0001$ (i.e.  1/10,000 have the disease)      \n",
    "$P(positive\\mid no disease)=0.01$ (i.e. he false positive rate is 1%. This means 1% of people who take the test will test positive, even though they do not have the disease)      \n",
    "\n",
    "Bayes’ Theorem\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A)\\, P(B | A)}{P(B)},\n",
    "$$\n",
    "\n",
    "which can be rewritten as  \n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A)\\, P(B | A)}{P(A)P(B|A)+P(\\bar{A})P(B|\\bar{A})},\n",
    "$$\n",
    "\n",
    "which in our example is\n",
    "$$\n",
    "P(disease|positive) = \\frac{P(disease)\\, P(positive | disease)}{P(disease)P(positive|disease)+P(no \\quad  disease)P(positive|no \\quad disease)},\n",
    "$$\n",
    "\n",
    "plugging in the numbers gives\n",
    "\n",
    "$$\n",
    "P(disease|positive)= \\frac{(0.0001)\\, (1)}{(0.0001)(1)+(0.9999)(0.01)}, \\approx 0.01\n",
    "$$\n",
    "\n",
    "So even though the test is 99% accurate, of all people who test positive, over 99% do not have the disease.  \n",
    "\n",
    "\n",
    "## Bayesians versus Frequentists\n",
    "\n",
    " \n",
    "[Frequentist inference](https://en.wikipedia.org/wiki/Frequentist_inference) or frequentist statistics is a scheme for making statistical inference based on the frequency or proportion of the data. This effectively requires that conclusions should only be drawn with a set of repetitions.    \n",
    "\n",
    "Frequentists will only generate statistical inference given a large enough set of repetitions. In contrast, a Bayesian approach to inference does allow probabilities to be associated with unknown parameters.   \n",
    "\n",
    "![Count Von Count](http://nikbearbrown.com/YouTube/MachineLearning/M10/Count_von_Count_kneeling.png)   \n",
    "*Count Von Count*   \n",
    "- from https://en.wikipedia.org/wiki/File:Count_von_Count_kneeling.png  \n",
    "\n",
    "While \"probabilities\" are involved in both approaches to inference, frequentist probability is essentially equivelent to counting. The Bayesian approach allows these estimates of probabilities to be based upon counting but also allows for subjective estimates (i.e. guesses) of prior probabilities.\n",
    "\n",
    "Bayesian probability, also called evidential probability, or subjectivist probability, can be assigned to any statement whatsoever, even when no random process is involved. Evidential probabilities are considered to be degrees of belief, and a Bayesian can even use an un-informative prior (also called a non-informative or Jeffreys prior).\n",
    "\n",
    "In Bayesian probability, the [Jeffreys prior](https://en.wikipedia.org/wiki/Jeffreys_prior), named after Sir Harold Jeffreys, is a non-informative (objective) prior distribution for a parameter space.  The crucial idea behind the Jeffreys prior is the Jeffreys posterior. This posterior aims to reflect as best as possible the information about the parameters brought by the data, in effect  \"representing ignorance\" about the prior. This is sometimes called the \"principle of indifference.\" Jeffreys prior is proportional to the square root of the determinant of the Fisher information:\n",
    "\n",
    "$$\n",
    "p\\left(\\vec\\theta\\right) \\propto \\sqrt{\\det \\mathcal{I}\\left(\\vec\\theta\\right)}.\\,\n",
    "$$\n",
    "\n",
    "It has the key feature that it is invariant under reparameterization of the parameter vector $\\vec\\theta.$  \n",
    "\n",
    "At its essence the Bayesian can be vague or subjective about an inital guess at a prior probability. and the the posterior probability be updated data point by data point. A Bayesian defines a \"probability\" in the same way that many non-statisticians do - namely an indication of the plausibility or belief of a proposition.\n",
    "\n",
    "A Frequentist is someone that believes probabilities represent long run frequencies with which events occur; he or she will have a model (e.g. Guassian, uniform, etc.) of how the sample popluation was generated. The observed counts are considered a random sample the estimate the true parameters of the model.   \n",
    "\n",
    "It is important to note that most Frequentist methods have a Bayesian equivalent (that is, they give the same results) when there are enough repeated trails. That is, they converge the the same result given enough data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Genetic Algorithms\n",
    "\n",
    "A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic is often used to generate useful solutions to optimization and search problems.\n",
    "\n",
    "![ genetic algorithm (GA)](http://nikbearbrown.com/YouTube/MachineLearning/M07/GA.png)\n",
    "*Genetic algorithm (GA)*  \n",
    "\n",
    "### Genetic Algorithm Pseudocode \n",
    "\n",
    "Create an initial population, typically random  \n",
    "\n",
    "While the best candidate so far is not a solution:  \n",
    "   Create new population using crossover and mutation.  \n",
    "   Evaluate the fitness of each candidate in the population.  \n",
    "   Replace/delete least-fit population  \n",
    "   \n",
    "Return the best candidate found\n",
    "\n",
    "\n",
    "### Genetic Algorithm Basic components\n",
    "\n",
    "* Candidate representation  \n",
    "    + Important to choose this well the form can effect the solution.  \n",
    "    + The typical candidate representation is a binary string.  \n",
    "* successor functions.  \n",
    "    + Mutation, crossover  \n",
    "    + Mutation - Given a candidate, return a slightly different candidate.  \n",
    "    + Crossover - Given two candidates, produce one that has elements of each.  \n",
    "* Fitness function  \n",
    "    + The fitness function quantitates estimates how close a candidate is to being a solution.  \n",
    "* Solution test  \n",
    "    + Check whether the candidate is a solution.\n",
    "* Some parameters  \n",
    "    + Population size  \n",
    "    + Generation limit  \n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "*Pros*  \n",
    "*  Fast (and low memory)  \n",
    "*  Finding a candidate representation and fitness function are the bulk of the work.  \n",
    "\n",
    "*Cons*   \n",
    "* Randomized (not guaranteed optimal or complete).   \n",
    "* Can get stuck on local maxima (crossover is intended to help get out of local maxima)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Simulated Annealing\n",
    "\n",
    "A Simulated Annealing (SA) is a  probabilistic search heuristic that mimics the process of cooling in thermodynamic systems. This heuristic is often used to generate useful solutions to optimization and search problems.  The method is an adaptation of the [Metropolis–Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published in a paper by N. Metropolis et al. in 1953.\n",
    "\n",
    "Basically simulated annealing perturbs the current solution  and then checks to see whether the new solution is good or not. If its an improvement it will accept it and if the new solution is worse it may accept it with a probability inversely proportional to how much worse the new solution changes the current one. Compared to pure gradient descent the main difference is that SA allows \"uphill\" steps. Simulated annealing also differs from gradient descent in that a move is selected at random.\n",
    "\n",
    "\n",
    "![gradient descent](http://nikbearbrown.com/YouTube/MachineLearning/M07/Gradient_Descent.png)\n",
    "*Gradient Descent*    \n",
    "\n",
    "### Metropolis–Hastings algorithm\n",
    "\n",
    "[Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) (symmetric proposal distribution)\n",
    "Let f(x) be a function that is proportional to the desired probability distribution P(x) \n",
    "\n",
    "\n",
    "\n",
    "This algorithm proceeds by randomly attempting to move about the sample space, sometimes accepting the moves and sometimes remaining in place.\n",
    "\n",
    "\n",
    "Perturb (randomly) the current state to a new state. \n",
    "$\\Delta E$ is the difference in energy between current and new state.   \n",
    "If $\\Delta E < 0$ (new state is lower), accept new state as current state\n",
    "If  $\\Delta E > 0$  accept new state with probability inversely proportional to the increase in system energy. Traditionally the change in [Gibbs free energy](https://en.wikipedia.org/wiki/Gibbs_free_energy) is used for thermodynamic free energy systems.\n",
    "\n",
    "This can be run for a fixed number of iterations or if the overall system energy can measured then it can be run until the overall system energy settles.\t\t\n",
    "\n",
    "\n",
    "### Simulated Annealing Pseudocode \n",
    "\n",
    "Simulated Annealing uses the Metropolis–Hastings algorithm with a temperature parameter $T$ that effects the acceptance probability of an \"uphill\" transition. At higher\"temperatures\" accpeting \"uphill\" transitions is more probable. The algorithm starts initially with $T$ set to a high value , and then it is decreased at each step following some annealing schedule—which is often specified by the user, but must end with $T=0$. At $T=0$ there is no chance of accpeting \"uphill\" transitions and so it becomes gradient descent.  \n",
    "\n",
    "At a fixed temperature T:  \n",
    "Perturb (randomly) the current state to a new state. \n",
    "$\\Delta E$ is the difference in energy between current and new state.  \n",
    "If $\\Delta E < 0$ (new state is lower), accept new state as current state\n",
    "If  $\\Delta E > 0$  accept new state with probability inversely proportional to the increase in system energy as a function of T.\n",
    "\n",
    "Eventually the systems evolves into thermal equilibrium at temperature T ; then the formula mentioned before holds When equilibrium is reached, temperature T can be lowered and the process can be repeated.\n",
    "\n",
    "\n",
    "## Travelling Salesman Problem\n",
    "\n",
    "The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.\n",
    "\n",
    "![Travelling Salesman Problem](http://nikbearbrown.com/YouTube/MachineLearning/M07/TSP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Self-Organizing Maps (SOM)\n",
    "\n",
    "A [self-organizing map (SOM)](https://en.wikipedia.org/wiki/Self-organizing_map) or self-organizing feature map (SOFM) is an artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map. This makes SOMs useful for visualizing low-dimensional views of high-dimensional data. In this sense, SOMs are similar to multidimensional scaling. Self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.\n",
    "\n",
    "![SOM Algorithm](http://nikbearbrown.com/YouTube/MachineLearning/M07/SOM_Algorithm.png)  \n",
    "*SOM Algorithm*  \n",
    "\n",
    "![SOM Training](http://nikbearbrown.com/YouTube/MachineLearning/M07/Som_Training.png)  \n",
    "*SOM Training*  \n",
    "\n",
    "### SOM neighborhood topology\n",
    "\n",
    "\n",
    "![Square Grid Lattice SOM](http://nikbearbrown.com/YouTube/MachineLearning/M07/Square_Grid_Lattice_SOM.png)  \n",
    "*Square Grid Lattice SOM]* \n",
    "\n",
    "![Hexoganal Honeycomb Lattice SOM](http://nikbearbrown.com/YouTube/MachineLearning/M07/Hexoganal_Honeycomb_Lattice_SOM.png)  \n",
    "*Hexoganal Honeycomb Lattice SOM]*  \n",
    "\n",
    "The grids have an $x$ dimension and $y$ dimension. The topology of the grid is rectangular or hexagonal. \n",
    "\n",
    "### SOM neighborhood functions\n",
    "\n",
    "While the neighborhood function $\\theta(u, v, s)$ usually depends on the lattice distance between the BMU (neuron u) and neuron v. Other functions such as a [Gaussian function](https://en.wikipedia.org/wiki/Gaussian_function) or a radius distance centered around the neuron are also common choices.   \n",
    "\n",
    "### SOM Algorithm  \n",
    "\n",
    "\n",
    "A. Randomize the map's nodes' weight vectors. The weights of the neurons are initialized either to small random values or sampled evenly from the subspace spanned by the two largest principal component eigenvectors.   \n",
    "\n",
    "B.  Grab an input vector $\\mathbf{D(t)}$. $t$ is the index of the target input data vector in the input data set $\\mathbf{D}$ Each data point, $\\mathbf{D(t)}$, needs to be mapped to a \"neuron.\"\n",
    "\n",
    "C.  Traverse each node in the map. Use the Euclidean distance or another similarity metric between the input vector $\\mathbf{D(t)}$ and the map's node's weight vector. The node that produces the smallest distance (i.e. the best matching unit, BMU) is associated with that neuron.\n",
    "\n",
    "D. Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector. This is analogous to updating the new means to be the centroids of the observations in the new clusters the means in [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering). That is,\n",
    "\n",
    "$$\n",
    "Wv(s + 1) = Wv(s) + \\theta(u, v, s) \t\\alpha(s)(D(t) - Wv(s))\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "* $s$ is the current iteration\n",
    "* $\\lambda$ is the iteration limit. Sometimes this is called the maximum number of 'epochs'\n",
    "* $v$ is the index of the node in the map\n",
    "* $\\mathbf{W_v}$ is the current weight vector of node v\n",
    "* $u$ is the index of the best matching unit (BMU) in the map\n",
    "* $\\Theta (u, v, s)$ is a restraint due to distance from BMU, usually called the neighborhood function,\n",
    "* $\\alpha (s)$ is a learning restraint due to iteration progress. Sometimes this is called the learning rate.\n",
    "\n",
    "E Increase $s$ and repeat from step B while $s < \\lambda$\n",
    "\n",
    "Note that the neighborhood function $\\Theta (u, v, s)$  depends on the lattice distance between the BMU (neuron u) and neuron v. In the simplest form it is 1 for all neurons close enough to BMU and 0 for others, but a Gaussian function is a common choice, too. Regardless of the functional form, the neighborhood function shrinks with time.  \n",
    "\n",
    "A variant algorithm:\n",
    "\n",
    "A. Randomize the map's nodes' weight vectors  \n",
    "B. Traverse each input vector in the input data set   \n",
    "C. Traverse each node in the map    \n",
    "D.  Use the Euclidean distance formula to find the similarity between the input vector and the map's node's weight vector\n",
    "E.  Track the node that produces the smallest distance (this node is the best matching unit, BMU)\n",
    "Update the nodes in the neighborhood of the BMU (including the BMU itself) by pulling them closer to the input vector\n",
    "$Wv(s + 1) = Wv(s) + \\theta(u, v, s) \\alpha(s)(D(t) - Wv(s))$\n",
    "F. Increase $s$ and repeat from step B while $s < \\lambda$\n",
    "\n",
    "\n",
    "The kind of training is called *[competitive learning](https://en.wikipedia.org/wiki/Competitive_learning)* n which nodes compete for the right to respond to a subset of the input data.\n",
    "\n",
    "\n",
    "### SOM Usage\n",
    "\n",
    "Using Self-Organising Maps are typcially done as follows:\n",
    "\n",
    "Select the size and type of the map. The shape can be hexagonal or square. Note that hexagonal grid has six immediate neighbors whereas a square usually has four. This topology determines the number of \"neurons.\"\n",
    "\n",
    "The algorithm then \n",
    "\n",
    "A. Initializes all node weight vectors.  \n",
    "B. Chooses a random data point from the training data to present to the SOM.  \n",
    "C. Finds the \"Best Matching Unit\" (BMU) in the map.\n",
    "Determine the nodes within the \"neighborhood\" of the BMU.  \n",
    "D. The size of the neighborhood decreases with each iteration.  \n",
    "E. Adjust weights of nodes (neurons) in the BMU neighborhood  towards a chosen datapoint.\n",
    "- The learning rate decreases with each iteration.  \n",
    "- The magnitude of the adjustment is proportional to the proximity of the node to the BMU.  \n",
    "\n",
    "\n",
    "Repeat Steps B-E for a fixed number of iterations or until convergence.\n",
    "\n",
    "### SOM Pros and Cons  \n",
    "\n",
    "Pros:\n",
    "\n",
    "* Intuitive method.  \n",
    "* Simple algorithm.    \n",
    "* New data points can be mapped to trained model for predictive purposes.   \n",
    "\n",
    "Cons:\n",
    "\n",
    "* Requires clean, numeric data.  \n",
    "\n",
    "\n",
    "## SOM Visualization  \n",
    "\n",
    "The Self-Organizing Maps projects high-dimensional data onto a two-dimensional map. The projection preserves the topology of the data so that similar data items will be mapped to nearby locations on the map. As such one of its great strengths is high-dimensional data visualization, akin to [multidimensional scaling (MDS)](https://en.wikipedia.org/wiki/Multidimensional_scaling).    \n",
    "\n",
    "### SOM Cluster Maps\n",
    "\n",
    "\n",
    "![SOM Clusters](http://nikbearbrown.com/YouTube/MachineLearning/M07/SOM_Clusters.png)  \n",
    "*SOM Clusters*   \n",
    "- from [Self-Organising Maps for Customer Segmentation using R](http://www.r-bloggers.com/self-organising-maps-for-customer-segmentation-using-r/) via @rbloggers\n",
    "\n",
    "\n",
    "![SOM Clusters with Labels](http://nikbearbrown.com/YouTube/MachineLearning/M07/SOM_Clusters_Lablels.png)  \n",
    "*SOM Clusters with Labels*   \n",
    "- from [Self-Organising Maps for Customer Segmentation using R](http://www.r-bloggers.com/self-organising-maps-for-customer-segmentation-using-r/) via @rbloggers\n",
    "\n",
    "![SOM Clusters on Map](http://nikbearbrown.com/YouTube/MachineLearning/M07/SOM_Clusters_on_Map.png)  \n",
    "*SOM Clusters on Map*    \n",
    "- from [Self-Organising Maps for Customer Segmentation using R](http://www.r-bloggers.com/self-organising-maps-for-customer-segmentation-using-r/) via @rbloggers\n",
    "### SOM Heatmaps\n",
    "\n",
    "A [heat map](https://en.wikipedia.org/wiki/Heat_map) is a graphical representation of data where a gradient of values are represented as colors.\n",
    "\n",
    "![SOM Heatmap](http://nikbearbrown.com/YouTube/MachineLearning/M07/SOM_Heatmap.png)  \n",
    "*SOM Heatmap*   \n",
    "- from [Self-Organising Maps for Customer Segmentation using R](http://www.r-bloggers.com/self-organising-maps-for-customer-segmentation-using-r/) via @rbloggers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data pipelines\n",
    "\n",
    "Data Pipeline is an embedded data processing engine. The engine runs inside your applications, APIs, and jobs to filter, transform, and migrate data in an automated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SQL\n",
    "\n",
    "SQL (pronounced \"ess-que-el\") stands for Structured Query Language. SQL is used to communicate with a [relational database](https://en.wikipedia.org/wiki/Relational_database).  A relational database is a digital database whose organization is based on the relational model of data, as proposed by E. F. Codd in 1970. The various software systems used to maintain relational databases are known as a relational database management system (RDBMS). Virtually all relational database systems use SQL (Structured Query Language) as the language for querying and maintaining the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NoSQL\n",
    "\n",
    "A [NoSQL](https://en.wikipedia.org/wiki/NoSQL) (originally referring to \"non SQL\" or \"non relational\") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. \n",
    "\n",
    "Motivations for this approach include: simplicity of design, spped, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases), and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MapReduce\n",
    "\n",
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.[\n",
    "\n",
    "A MapReduce program is composed of a Map() procedure (method) that performs filtering and sorting (such as sorting students by first name into queues, one queue for each name) and a Reduce() method that performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). \n",
    "\n",
    "The \"MapReduce System\" (also called \"infrastructure\" or \"framework\") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "I highly recommend _Deep Learning - Adaptive Computation and Machine Learning series by Ian Goodfellow, Yoshua Bengio,\n",
    "and Aaron Courville_ [https://github.com/HFTrader/DeepLearningBook](\n",
    "https://github.com/HFTrader/DeepLearningBook)\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "[6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "[Artificial neural networks (ANNs)](https://en.wikipedia.org/wiki/Artificial_neural_network) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains.\n",
    "\n",
    "![neural network](http://nikbearbrown.com/YouTube/MachineLearning/DATA/neural_network.svg.png)\n",
    "\n",
    "**Neural networks** consist of:\n",
    "\n",
    "_Input layer_\n",
    "\n",
    "Neurons that maps the input to a neural network.\n",
    "\n",
    "_Output layer_\n",
    "\n",
    "Neurons that maps the output to a neural network. Often converts to output to a probability.\n",
    "\n",
    "*\tSoftmax (If you want the outputs of a network to be interpretable as posterior probabilities.)\n",
    "*\tCros\n",
    "\n",
    "_Activation functions_\n",
    "\n",
    "The [activation function](https://en.wikipedia.org/wiki/Activation_function) of a node defines the output of that node given an input or set of inputs.\n",
    "\n",
    "*\tRectified linear unit (ReLU)\n",
    "*\tTanH\n",
    "*\tLeaky rectified linear unit (Leaky ReLU)\n",
    "*\tParameteric rectified linear unit (PReLU)\n",
    "*\tRandomized leaky rectified linear unit (RReLU)\n",
    "*\tExponential linear unit (ELU)\n",
    "*\tScaled exponential linear unit (SELU)\n",
    "*\tS*shaped rectified linear activation unit (SReLU)\n",
    "*\tIdentity\n",
    "*\tBinary step\n",
    "*\tLogistic\n",
    "*\tArcTan\n",
    "*\tSoftsign\n",
    "*\tAdaptive piecewise linear (APL)\n",
    "*\tSoftPlus\n",
    "*\tSoftExponential\n",
    "*\tSinusoid\n",
    "*\tSinc\n",
    "*\tGaussian\n",
    "\n",
    "_Comparison of activation functions_\n",
    "\n",
    "Some desirable properties in an activation function include:\n",
    "* Nonlinear – When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.  \n",
    "* Continuously differentiable – This property is necessary for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.  \n",
    "* Range – When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.  \n",
    "* Monotonic – When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.\n",
    "* Smooth Functions with a Monotonic derivative – These have been shown to generalize better in some cases. The argument for these properties suggests that such activation functions are more consistent with Occam's razor.  \n",
    "* Approximates identity near the origin – When activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.  \n",
    "\n",
    "_Objective function (cost function or loss function)_\n",
    "\n",
    "The objective function estimates the error. (Is minimized during training)\n",
    "\n",
    "*\tQuadratic cost (mean*square error)\n",
    "*\tCross*Entropy\n",
    "\n",
    "_Back-Propagation_\n",
    "\n",
    "[Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) is a method used in artificial neural networks to calculate the error contribution of each neuron after a batch of data. This is used by an enveloping optimization algorithm to adjust the weight of each neuron, completing the learning process for that case.\n",
    "\n",
    "Technically it calculates the gradient of the loss function. It is commonly used in the gradient descent optimization algorithm. It is also called backward propagation of errors, because the error is calculated at the output and distributed back through the network layers.\n",
    "\n",
    "_Epochs_\n",
    "\n",
    "One epoch = one forward pass and one backward pass. \n",
    "\n",
    "*\tnumber of epochs\n",
    "\n",
    "_Gradient estimation_\n",
    "\n",
    "*\tstochastic gradient descent\n",
    "*\tAdagrad\n",
    "*\tRMSProp\n",
    "*\tADAM\n",
    "*\tNAG\n",
    "*\tAdadelta\n",
    "*\tMomentum\n",
    "\n",
    "_Batch size_\n",
    "\n",
    "batch size = the number of training examples in one forward/backward pass. It's much faster to estimate the gradient on a sample than to calculate it over the entire data set.\n",
    "\n",
    "_Batch normalization_\n",
    "\n",
    "Normalization (shifting inputs to zero-mean and unit variance) is often used as a pre-processing step to make the data comparable across features. \n",
    "\n",
    "Because a multiplies across the weights over each layer this can change the distribution. This is called _internal covariate shift._ As the data flows through a deep network, the weights and parameters adjust those values, sometimes making the data too big or too small again.\n",
    "\n",
    "Internal covariate shift refers to the change in the input distribution to a learning system. In the case of deep networks, the input to each layer is affected by parameters in all the input layers.\n",
    "\n",
    "Rather than just performing normalization once in the beginning, you're doing it through the process. \n",
    "\n",
    "_Regularization_\n",
    "\n",
    "*\tL1\n",
    "*\tL2\n",
    "*\tDrop out\n",
    "\n",
    "\n",
    "### Shallow Neural Networks\n",
    "\n",
    "_Shallow Neural Networks_ refers to neural networks with few hidden layers. \n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "_Deep Learning_\n",
    "\n",
    "Deep feedforward networks feedforward , also often called neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models. The goal of a feedforward network is to approximate some function $f(x)$.\n",
    "\n",
    "Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as \"deep learning\", although deep learning is not strictly synonymous with deep neural networks it often refers to th neural networks with many hidden layers. \n",
    "\n",
    "### Convolutional Networks\n",
    "\n",
    "A convolutional networks (CNN) consists of an input and an output layer, as well as multiple hidden layers. The hidden layers are either convolutional, pooling or fully connected.\n",
    "\n",
    "Convolutional layers apply a convolution operation to the input, passing the result to the next layer. The convolution emulates the response of an individual neuron to visual stimuli.\n",
    "\n",
    "Each convolutional neuron processes data only for its receptive field. Tiling allows CNNs to tolerate translation of the input image (eg. translation, rotation, perspective distortion).\n",
    "\n",
    "Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary even in a shallow architecture (opposite of deep). The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with many fewer parameters. In other words, it resolves the vanishing or exploding problems in training traditional multi-layer neural networks with many layers by using backpropagation.\n",
    "\n",
    "### Sequence Modeling: Recursive neural networks\n",
    "\n",
    "[Recursive neural networks (RNNs)](https://en.wikipedia.org/wiki/Recursive_neural_network) is a kind of deep neural network created by applying the same set of weights recursively over a structure, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful for instance in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms.\n",
    "\n",
    "\n",
    "### Autoencoders\n",
    "\n",
    "An autoencoder, autoassociator or Diabolo network is an artificial neural network used for unsupervised learning of efficient codings. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.\n",
    "\n",
    "Autoencoder’s though are difficult to interpret of the representation of semantics and aren’t really a generative model.\n",
    "\n",
    "Autoencoder\n",
    "i.\tencoder and dencoder  \n",
    "ii.\thidden layers\n",
    "iii.\tbottleneck layer - forces network to learn a compressed latent representation\n",
    "iv.\treconstruction loss  - forces hidden layer to represent info about the input\n",
    ")\n",
    "\n",
    "### Variational autoencoders (VAEs)\n",
    "\n",
    "Variational autoencoders are a stochastic variational extension of autoencoders  that allow for a probabilistic representation of the data and amortized inference.\n",
    "\n",
    "In a VAE, the encoder becomes a variational inference network that maps the data to the a distribution for the hidden variables, and the decoder becomes a generative network that maps the latent variables back to the data. In just a couple of years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions.\n",
    "\n",
    "Variational Autoencoders (VAEs) have some very desirable properties:\n",
    "\n",
    "i.\tClear semantics as a generative model\n",
    "ii.\tExtension of autoencoders that allow sampling and estimating probabilities.\n",
    "iii.\tThis creates a kind of implicit generative model it creates a latent representation thru its probabilities\n",
    "iv.\t\"Latent variables\" has a fixed prior distribution\n",
    "v.\tProbabilistic encoder and dencoder\n",
    "vi.\tProbabilistic representation of the data\n",
    "\n",
    "Variational autoencoders have a clear advantage over autoencoders in that the probabilistic representation of the data is a form of representation learning.\n",
    "\n",
    "\n",
    "_Autoregresstive variational autoencoders_\n",
    "\n",
    "Standard VAE’s have some issues:\n",
    "i.\tThey do not encode what is not useful for them to decode.  This means they don't capture fine details. Subtle features (e.g. small nodules that may be important clinically but don’t represent much of the image space may be missed.)\n",
    "\n",
    "\n",
    "Autoregresstive VAE’s use autoregresstive networks in the encoder and the dencoder to capture more local info in the estimation of the densities.\n",
    "\n",
    "\n",
    "\n",
    "### Restricted Boltzmann machines (RBMs)\n",
    "\n",
    "Restricted Boltzmann machines (RBM)s are a very simple model, just a fully connected bipartite graph with an input layer and a hidden layer. The cost function minimizes an energy function by re-weighting to minimize the difference between the input layer and the hidden layer.\n",
    "\n",
    "\n",
    "Like VAE’s restricted Boltzmann machines can be thought as a form of representation learning.\n",
    "\n",
    "Restricted Boltzmann machines:\n",
    "i.\tClear semantics as a generative model\n",
    "ii.\tSimple, just a fully connected bipartite graph with an input layer and a hidden layer.\n",
    "iii.\tMinimizes an energy function (re-weighting to minimize the difference between the input layer and the hidden layer).\n",
    "iv.\tEnergies can easily be converted to probabilities.\n",
    "v.\tCreates a latent representation thru its probabilities.\n",
    "\n",
    "\n",
    "### Deep Generative Models\n",
    "\n",
    "Deep Generative Models collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update September 5, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
